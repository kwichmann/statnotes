\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png}

\title{Artificial Neural Networks}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Feed-forward artificial neural networks}
An \textit{artificial neural network} (ANN) superficially mimics the workings of the brain by being made up of \textit{neurons} and \textit{dendrites/axons}\footnote{In real neural networks, dendrites carry inputs to the neuron, while axons carry outputs from the neuron.}.

Here we will focus on \textit{feed-forward neural networks}. Here, the neurons are organized into \textit{layers}. These come in three categories:
\begin{itemize}
\item The \textit{input layer}, into which the data to be processed enters.
\item \textit{Hidden layers} - which are layers between the input and output layers.
\item The \textit{output layer} in which the result of the ANN's data processing can be read.
\end{itemize}

Every neuron in each layer is connected to every neuron in the next layer. A feed-forward ANN has at least three layer: Input, output and at least one hidden layer. An ANN with two or more hidden layers is known as a \textit{deep network}. 

Figure \ref{fig:ann_example} shows a neural network with three laters: 3 cells in the input layer, 4 cells in the hidden layer, and 2 cells in the output layer.

\begin{figure}
\centering
\includegraphics{artificial_neural_network}
\caption{An ANN with three layers.}
\label{fig:ann_example}
\end{figure}

The cells in each layer are connected by dendrites/axons, which are represented as arrows in figure \ref{fig:ann_example}: Every cell in each layer is connected to all the cells in the next layer.

As the data put into the input layer is translated into \textit{activity} levels in the cells. These activities then propagate via the axons through the network, causing different activity levels in each layer. The is repeated, until reaching the conclusion: The activity levels in the output layer. These are then interpreted accordingly.

\section{Neuron modelling}

The activity of a neuron is affected by all dendrites pointing to the cell - for a feed-forward neural network, this means all cells in the previous layer. In other words, the activity of the neuron depends on these inputs.

\subsection{Types of neurons: Deterministic}
The actual activity depends on the type of neuron used. In this section we'll look at \textit{deterministic} neurons, i.e. neurons where the inputs uniquely determine the activity. In other words, the activity is a function of the input. We call this the \textit{activation function}.

Here, we will only consider functions that depends on a weighted sum of the inputs: All the inputs are weighted corresponding to the strength of each dendrite. In addition a \textit{bias} term may be added - a constant term. The actual activity is found by applying the activation function to this quantity. Mathematically, this may be formulated:
\begin{equation}
\label{neuron_activity}
a_\textrm{neuron}=f\left(\sum_{i=1}^n \theta_i x_i + x_0\right)
\end{equation}
Here, $a$ is the activation, the $x_i$ are the inputs ($n$ total), the $\theta_i$ the corresponding weights (strength of the dendrites), $x_0$ is the bias term and finally $f$ is the activation function.

\begin{figure}
\centering
\includegraphics[width=50mm]{neuron}
\caption{Neuron with three inputs and a bias term}
\label{fig:neuron}
\end{figure}

The situation is outlined in figure \ref{fig:neuron}. Note that the bias has been shown as an input, even if this is not really the case: It is merely a constant term.

The inputs and weights are often written in vector form:
\begin{equation}
x=\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix},
\quad
\theta=\begin{pmatrix}
\theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
\end{pmatrix}
\end{equation}
Hence, equation \ref{neuron_activity} may be written:
\begin{equation}
a_\textrm{neuron}=f\left(\theta^T x + x_0\right)
\end{equation}

\subsubsection{Linear neurons}
Here, the activation function is simply $f(z)=z$.

\subsubsection{Binary threshold neurons}
For these types of neurons, the activation function is a \textit{step function}, i.e. a function of the form:
\begin{equation}
f(z)=
\begin{cases}
0 & \textrm{if } z < c \\
1 & \textrm{if } z \ge c
\end{cases}
\end{equation}
Here $c$ is the \textit{activation threshold}. By adjusting the bias term, $c$ can always be turned into zero.

This type of neuron is the basis for the learning model known as the \textit{perceptron}.

\subsubsection{Rectified linear neurons}
This combines the two previous types of neurons by setting:
\begin{equation}
f(z)=
\begin{cases}
0 & \textrm{if } z < 0 \\
z & \textrm{if } z \ge 0
\end{cases}
\end{equation}
These are also sometimes known as \textit{linear threshold neurons}.

\subsubsection{Sigmoid neurons}
Here, the activation function is a \textit{sigmoid function}, which just means s-shaped. This may be any function that is convex for $z<0$ and concave for $z>0$, and which has finite limits for $z\rightarrow\pm\infty$.

The most famous of these functions is the \textit{logistic function}, which is the one we will focus on in the following. In fact it's common to simply denote it as \textbf{the} sigmoid function. The next section is devoted to the details of this function.

Another example are tanh-neurons, for which the activation function unsurprisingly is:
\begin{equation}
f(z)=\tanh(z)
\end{equation} 

\subsection{Stochastic binary neurons}
The deterministic neurons always have the same activity given the same inputs. This is not true for stochastic binary neurons. Instead, the values of the activation function $f(z)$ are taken as \textit{probabilities}. More precisely, it is the probability of the activity being 1. Otherwise, the activation is 0 (hence the 'binary' in the name).

This usually assumes that $f$ only takes on values between 0 and 1, so that it may be interpreted as a probability.

\section{The logistic function}
As noted above, we will consider sigmoid neurons for the rest of this text. For our activation function, we will choose the \textit{logistic function}. It is defined as:
\begin{equation}
\label{sigmoid}
g(z)=\frac{1}{1+e^{-z}}
\end{equation}
The function is graphed in figure \ref{fig:sigmoid}. It can be though of as a "smoothed out step function".

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = $z$,
    ylabel = {Activation},
    ymin = 0,
    ymax = 1.2,
]
\addplot [
    domain=-6:6, 
    samples=50, 
    color=red,
]
{1/(1+exp(-x)};
\addlegendentry{$g(z)$}
\end{axis}
\end{tikzpicture}
\caption{The sigmoid function.}
\label{fig:sigmoid}
\end{figure}

The reason for the choice of this function is two-fold. First of all, it introduces \textit{activation saturation} into the model: Even if $\theta^T x+x_0$ turns out to be very large, the corresponding activity will still be close to 1. Similarly large negative values produce activities close to 0.

However, many functions could have achieved activation saturation. But $g(z)$ also has an additional nice mathematical property. Consider its derivative:
\begin{equation}
g'(z)=\frac{1'\cdot(1+e^{-z})-1\cdot(1+e^{-z})'}{(1+e^{-z})^2}=\frac{e^{-z}}{(1+e^{-z})^2}=\frac{e^{-z}}{1+e^{-z}}g(z)
\end{equation}
It turns out the first factor is equal to $1-g(z)$:
\begin{equation}
1-g(z)=1-\frac{1}{1+e^{-z}}=\frac{1+e^{-z}}{1+e^{-z}}-\frac{1}{1+e^{-z}}=\frac{1+e^{-z}-1}{1+e^{-z}}=\frac{e^{-z}}{1+e^{-z}}
\end{equation}
So we have:
\begin{equation}
\label{sigmoid_derivative}
g'(z)=g(z)\left(1-g(z)\right)
\end{equation}

\section{Matrix form of layer activities}
So, given cell number $i$ in layer $L$, its activation depends on the input activities from layer $L-1$, the activity is:
\begin{equation}
\label{layer_activity}
a^{(L)}_i=g\left(\left(\theta^{(L)}_i\right)^T a^{(L-1)} + b_i^{(L)}\right)
\end{equation}
Here, the activities from layer $L-1$ has been collected into the vector $a^{(L-1)}$ and the biases into the vector $b^{L}$. However, this may simply be expressed through matrix multiplication:
\begin{equation}
a^{(L)}=g\left(\theta^{(L)}a^{(L-1)}+b^{(L)}\right)
\end{equation}
Here, the matrix $\theta^{(L)}$ has rows consisting of weights for each cell. Also, $g$ is to be applied component-wise.

\subsection{Dimensionalities}
If layer $L-1$ contains $m_{L-1}$ cells and layer $L$ contains $m_L$ cells, this means the dimensionality of the matrices and vectors above are:
\begin{equation}
a^{(L-1)}\in\mathbb{R}^{m_{L-1}\times 1},\quad a^{(L)}, b^{(L)}\in\mathbb{R}^{m_L\times 1},\quad\theta^{(L)}\in\mathbb{R}^{m_L\times m_{L-1}} 
\end{equation}

\subsection{Output layer as a function of input layer}
If there is a total of $K$ layers in the ANN. Then the input layer is $x\equiv a^{(1)}$ and the output layer $y\equiv a^{(K)}$. Then we may express $y$ as a function of $x$ through repeated application of equation \ref{layer_activity}. This process is known as \textit{forward propagation}:
\begin{equation}
\label{hypothesis}
y=g\Bigg(\theta^{(K)}\bigg(\cdots g\Big(\theta^{(3)} g\big(\theta^{(2)}x+b^{(2)}\big)+b^{(3)}\Big)\cdots\bigg)+b^{(K)}\Bigg)\equiv h^{(\theta,b)}
\end{equation}
Here the notation $h^{(\theta,b)}$ been introduced. Here $h$ stands for \textit{hypothesis}, i.e. the hypothesis that the particular choices of $\theta$ and $b$ are appropriate.

\section{ANN used for classification}
As an example, consider an ANN trained for picture classification. Se we imagine an ANN which has a representation of a picture for inputs $x$. The output layer is a series of $n$ numbers between 0 and 1. We may interpret these as probabilities. Maybe an ANN has been trained to recognize cats, dogs and horses. In this case, the size of the output layer is 3, and $y_1, y_2$ and $y_3$ are interpreted as the probabilities of the input picture containing a cat, dog or a horse respectively. If we know the picture only contains one animal, we would simply pick the highest value and use the corresponding animal as prediction.

\subsection{Training an ANN - overview}
So the question is now how to train an ANN for classification? This is a supervised machine learning problem, as we imagine we have a large dataset to train the network on.

The actual training is done using \textit{maximum likelihood estimation} of the weights and biases. In practice, this will be done using gradient or a similar optimization algorithm. The partial derivatives needed for this is found using what is known as the \textit{backpropagation algorithm}.

All of these topics will be the subject of the next set of sections.

\section{Likelihood of a Bernoulli process}
Let's take a step back and consider a Bernoulli process, i.e. an experiment with two outcomes: success and failure. The probability of success is called $p$, and hence the probability of failure must be $1-p$. This may be summed up in the equation:
\begin{equation}
P(x|p)=p^x(1-p)^{1-x},\quad x=0,1
\end{equation}
Here $x=1$ indicates success and $x=0$ failure.

The "probability mindset" here is, that if we know $p$ we know the distribution. The idea of likelihood swaps this around: Instead we ask what the parameter $p$ might be given the outcome $x$:
\begin{equation}
\mathcal{L}(p|x)=p^x(1-p)^{1-x}
\end{equation}
Based on the observation, we will then pick the value of $p$ that makes the observation most likely. This is known as maximum likelihood estimation\footnote{For a single observation this means the estimate is $p=1$ in case of success and $p=0$ in case of failure. In other words $p=x$.}.

Often, it is more convenient to minimize the \textit{log-likelihood}, i.e. minus the logarithm of the likelihood\footnote{Not all presentations include the minus sign. It is kept here to make $L$ a cost function.}. Since $\log$ is monotone, this is equivalent. For the Bernoulli process:
\begin{equation}
L(p|x)=-\log\mathcal{L}(p|x)=-x\log p-(1-x)\log(1-p)
\end{equation}
This may also be viewed as the \textit{cross-entropy} between the distributions of $x$ and $p$.

$L$ can also be seen as a \textit{cost function}, i.e. something we wish to minimize. Figure \ref{fig:bernoulli} graphs the function for the two values of $x$. Because of the connection to entropy it is sometimes knows as the \textit{cross-entropy loss function}.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = $p$,
    ylabel = {Log-likelihood},
    ymin = 0,
    ymax = 2,
]
\addplot [
    domain=0:1, 
    samples=50, 
    color=red,
]
{-ln(x)};
\addlegendentry{$x=1$}

\addplot [
    domain=0:1, 
    samples=50, 
    color=green,
]
{-ln(1-x)};
\addlegendentry{$x=0$}

\end{axis}
\end{tikzpicture}
\caption{Log-likelihoods for the Bernoulli process.}
\label{fig:bernoulli}
\end{figure}

\section{Single item training set}
Now consider a single item training set, i.e. an input $x$ that has been classified as option $a$. This corresponds to a Bernoulli process for each cell in the output $y$, with only the $a$'th one being a success. So the desired output is $y=\delta_a$.

Now, the log-likelihood of the $i$'th such Bernoulli process depends on the actual choice of weights and biases - more specifically, it depends on the $h^{(\theta,b)}$ function defined in equation \ref{hypothesis}:
\begin{equation}
L_i(\theta,b|y_i)=-y_i\log\left[h^{(\theta,b)}_i\right]-(1-y_i)\log\left[1-h^{(\theta,b)}_i\right]
\end{equation}
If the processes are assumed to be independent (a big if, but let's go with that for now), the log-likelihoods add up, and the total likelihood is:
\begin{equation}
L(\theta,b|y)=\sum_i\left\{-y_i\log\left[h^{(\theta,b)}_i\right]-(1-y_i)\log\left[1-h^{(\theta,b)}_i\right]\right\}
\end{equation}
The task is now to find the weights and biases that minimize this. To do so, we wish to use gradient descent, and so need the partial derivatives of $L$ with respect to all weights and biases!

If $\alpha$ is any weight or bias, we can differentiate $L$ with respect to it:
\begin{equation}
\frac{\partial L}{\partial\alpha}=\sum_i\left\{-y_i\frac{1}{h^{(\theta,b)}_i}\frac{\partial h^{(\theta,b)}_i}{\partial\alpha}-(1-y_i)\frac{1}{1-h^{(\theta,b)}_i}\left(-\frac{\partial h^{(\theta,b)}_i}{\partial\alpha}\right)\right\}
\end{equation}
Collecting terms:
\begin{align}
\frac{\partial L}{\partial\alpha}=&\sum_i\left\{\frac{-y_i\left(1-h^{(\theta,b)}_i\right)+(1-y_1)h^{(\theta,b)}_i}{h^{(\theta,b)}_i(1-h^{(\theta,b)}_i)}\frac{\partial h^{(\theta,b)}_i}{\partial\alpha}\right\}\\
&\sum_i\left\{\frac{h^{(\theta,b)}_i-y_i}{h^{(\theta,b)}_i\left(1-h^{(\theta,b)}_i\right)}\frac{\partial h^{(\theta,b)}_i}{\partial\alpha}\right\}
\label{general_partial_formula}
\end{align}
So the problem is reduced to finding partial derivatives of $h$.

\subsection{Partial derivatives: Output layer}
The values of the output layer can be expressed through their dependence on the last hidden layer as per equation \ref{layer_activity}:
\begin{equation}
h^{(\theta,b)}=a^{(K)}=g(\theta^{(K)}a^{(K-1)}+b^{(K)})
\end{equation}
Remember that this is a vector equation - we need to consider each component. The property of the sigmoid function expressed in equation \ref{sigmoid_derivative} now comes in handy when we want to calculate partial derivatives:
\begin{equation}
\frac{\partial h^{(\theta,b)}_i}{\partial\alpha}=g(z_i)(1-g(z_i))\frac{\partial z_i}{\partial\alpha},\quad z_i=\sum_j\theta^{(K)}_{ij}a^{(K-1)}_j+b^{(K)}_i
\end{equation}
But $g(z_i)=h^{(\theta,b)}_i$! So inserting into equation \ref{general_partial_formula} there's a lot of cancellation:
\begin{equation}
\frac{\partial L}{\partial\alpha}=\sum_i\left\{\left(h^{(\theta,b)}_i-y_i\right)\frac{\partial z_i}{\partial\alpha}\right\}=\sum_i\delta^{(K)}_i\frac{\partial z_i}{\partial\alpha}
\label{general_partial_formula2}
\end{equation}
Here, the \textit{error} $\delta^{(K)}=h^{(\theta,b)}_i-y_i$ for the output layer has been introduced. This can be seen as the amount the hypothesis specified by the current weights and biases is off compared to what is desired. Be careful to distinguish between the error and Kronecker deltas in the following!

Equation \ref{general_partial_formula2} may be written in vector form:
\begin{equation}
\frac{\partial L}{\partial\alpha}=\left(\delta^{(K)}\right)^T\frac{\partial z}{\partial\alpha}
\end{equation}

Finally, we need to find the partial derivatives of $z$. Again, remember that $z$ is a vector:
\begin{equation}
z_i=\sum_j\theta^{(K)}_{ij}a^{(K-1)}_j+b^{(K)}_i
\end{equation}
At this point we only consider weights and biases from the axons pointing at the output layer, i.e. with superscript $K$. But since the weights form a matrix and the biases a vector, the resulting derivative objects will be 3 and 2-dimensional respectively:
\begin{equation}
\frac{\partial z_i}{\partial\theta^{(K)}_{kl}}=\sum_j\delta_{ik}\delta_{jl}a^{(K-1)}_j=\delta_{ik}a^{(K-1)}_l,\quad\frac{\partial z_i}{\partial b^{(K)}_k}=\delta_{ik}
\end{equation}
This means that the only non-zero derivatives are:
\begin{equation}
\frac{\partial z_i}{\partial\theta^{(K)}_{ij}}=a^{(K-1)}_j,\quad\frac{\partial z_i}{\partial b^{(K)}_i}=1
\end{equation}
We may now calculate the derivatives of $L$:
\begin{align}
\label{weight_derivative}
\frac{\partial L}{\partial\theta^{(K)}_{ij}}=&\sum_k\delta^{(K)}_k\delta_{ki}a^{(K-1)}_j=\delta^{(K)}_i a^{(K-1)}_j\\
\label{bias_derivative}
\frac{\partial L}{\partial b^{(K)}_i}=&\sum_k\delta^{(K)}_k\delta_{ki}=\delta^{(K)}_i
\end{align}

\subsection{Partial derivatives: Hidden layers}
On to the hidden layers. Luckily, it turns out that most of the calculations echo the output layer one closely.

Let's look at a weight or bias $\alpha$ from the last hidden layer. We need the derivative of $h$ with respect to $\alpha$. It is convenient to make the following definition:
\begin{equation}
z^{(N)}=\theta^{(N)}a^{(N-1)}+b^{(N)}
\end{equation}
Then we may write:
\begin{equation}
h^{(\theta,b)}=g(z^{(K)})
\end{equation}
Now we wish to form the partial derivative with respect to a weight/bias $\alpha$ from the last hidden layer, i.e. layer $K-1$. Here in vector form:
\begin{equation}
\frac{\partial h^{(\theta,b)}}{\partial\alpha}=g(z^{(K)})\odot(1-g(z^{(K)})\odot\frac{\partial z^{(K)}}{\partial\alpha}
\end{equation}
Here, the symbol $\odot$ is used to mean the \textit{Hadamard product} or \textit{Schur product}. It simply takes two vectors of equal length and return another vector of the same length, where each component is the product of the relevant factor components:
\begin{equation}
\begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_n
\end{pmatrix}
\odot
\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{pmatrix}
=
\begin{pmatrix}
a_1 b_1 \\ a_2 b_2 \\ \vdots \\ a_n b_n
\end{pmatrix}
\end{equation}
So what is the derivative?
\begin{equation}
\frac{\partial z^{(K)}}{\partial\alpha}=\theta^{(K)}\frac{\partial a^{(K-1)}}{\partial\alpha}
\end{equation}
But $a^{(K-1)}=g\left(z^{(K-1)})\right)$:
\begin{equation}
\frac{\partial}{\partial\alpha}g(z^{(K-1)})=g(z^{(K-1)})\odot(1-g(z^{(K-1)}))\odot\frac{\partial z^{(K-1)}}{\partial\alpha}
\end{equation}

Again, the two first factors can be expressed as activiations, and because $\alpha$ is associated with layer $K-1$, the derivative is analogous to the results from equations \ref{weight_derivative} and \ref{bias_derivative}. Inserting into equation \ref{general_partial_formula}, once again, there's some cancelling out, but this time we get some extra factors:
\begin{equation}
\sum_i\left\{\delta^{(K)}_i\sum_j\theta^{(K)}_{ij}a_j^{(K-1)}\left(1-a_j^{(K-1)}\right)\frac{\partial z^{(K-1)}_j}{\partial\alpha}\right\}
\end{equation}
Reorder the summation and use that $A_{ij}=A^T_{ji}$:
\begin{equation}
\frac{\partial L}{\partial\alpha}=\sum_{j}\left\{\underbrace{\sum_i\left(\theta^{(K)}\right)^T_{ji}\delta^{(K)}_i}\ a_j^{(K-1)}\left(1-a_j^{(K-1)}\right)\frac{\partial z^{(K-1)}_j}{\partial\alpha}\right\}
\end{equation}
The underbraced part is equal to $\left[\left(\theta^{(K)}\right)^T\delta^{(K)}\right]_j$. So it makes sense to set the error of layer $K-1$ equal to:
\begin{equation}
\label{backpropagating_errors}
\delta^{(K-1)}=\left(\theta^{(K)}\right)^T\delta^{(K)}\odot a^{(K-1)}\odot\left(1-a^{(K-1)}\right)
\end{equation}
The calculations are now essentially the same as in the last section, and we get:
\begin{align}
\frac{\partial L}{\partial\theta^{(K-1)}_{ij}}=&\delta^{(K-1)}_i a^{(K-2)}_j\\
\frac{\partial L}{\partial b^{(K-1)}_i}=&\delta^{(K-1)}_i
\end{align}
If $\alpha$ had been associated with an earlier hidden layer, we could have followed the same steps iteratively, defining the error of layer $N$ recursively:
\begin{equation}
\delta^{(N)}=\delta^{(N+1)}=\left(\theta^{(N+1)}\right)^T\delta^{(N+1)}\odot a^{(N)}\odot\left(1-a^{(N)}\right)
\end{equation}
The general formulas for the derivatives is then:
\begin{align}
\frac{\partial L}{\partial\theta^{(N)}_{ij}}=&\delta^{(N)}_i a^{(N-1)}_j\\
\frac{\partial L}{\partial b^{(N)}_i}=&\delta^{(N)}_i
\end{align}
Again, $N$ should be at least 2, reflecting the fact that there's no error, weights or biases for the input layer.

These can be summed up in matrix form:
\begin{align}
\label{general_derivative}
\frac{\partial L}{\partial\theta^{(N)}}=&\delta^{(N)}\left(a^{(N-1)}\right)^T\\
\frac{\partial L}{\partial b^{(N)}}=&\delta^{(N)}
\end{align}


\subsection{Backpropagation algorithm}
As we have seen above, calculation of derivatives is a recursive process, starting from the output layer and working backwards. Hence the name \textit{backpropagation algorithm}. Here are the steps in bullet form:
\begin{itemize}
\item First use forward propagation to calculate all the activations of the neurons corresponding to the input $x$ and the current weights and biases.
\item Using the $y$'s, calculate the error of the output layer.
\item Repeat the following until all derivatives are calculated:
\begin{itemize}
\item Calculate derivatives for the current layer using the layer's error and the previousy calculated activations.
\item Iteratively calculate the error of the previous layer.
\end{itemize}
\end{itemize}
This is merely one step in the gradient descent process. The derivatives are used to generate a new set of weights/biases and the process is repeated until all derivatives are sufficiently small.

\section{Full training set}
So far, we've been dealing with the situation for a training set consisting of only one item. In practice, we ideally want to train our ANN on a large dataset.

However, since each item in the training set is independent of each other, the log-likelihood of multiple items is simply equal to the sum of all of them. By additivity of the derivative, the backpropagation process essentially remains the same, except we need to sum over the entire training set in each step of the gradient descent.

\subsection{Vectorization}
If there's $p$ data points in the training set, we may collect them all into a $(n+1)\times p$ matrix $X$:
\begin{equation}
\label{row_of_ones}
X=
\begin{pmatrix}
1	&	\cdots	& 1	\\
|	&	\cdots	& |	\\
x_1	&	\cdots	& x_p \\
|	&	\cdots	& |
\end{pmatrix}
\end{equation}
Here, a constant coordinate of 1 have been added to each data point in order to unify treatment of weights and biases. This is done by setting:
\begin{equation}
\Theta^{(L)}=
\begin{pmatrix}
b^{(L)}_1		& \theta^{(L)}_{11}	& \theta^{(L)}_{12}	& \cdots	& \theta^{(L)}_{1n_{L-1}} \\
b^{(L)}_2		& \theta^{(L)}_{21}	& \theta^{(L)}_{22}	& \cdots 	& \theta^{(L)}_{2n_{L-1}} \\
\vdots			& \vdots		& \vdots		& \ddots 	& \vdots \\
b^{(L)}_{n_L}	& \theta^{(L)}_{n_L1}	& \theta^{(L)}_{n_L2}	& \cdots	& \theta^{(L)}_{n_Ln_{L-1}}
\end{pmatrix}
\end{equation}
So $\Theta^{(L)}\in\mathbb{R}^{n_L\times(n_{L-1}+1)}$. So we can get the activities in the first hidden layer by:
\begin{equation}
A^{(2)}=\Theta^{(2)}X=
\begin{pmatrix}
| & \cdots & | \\
a^{(2)}_1 & \cdots & a^{(2)}_p \\
| & \cdots & | 
\end{pmatrix}
\end{equation}
So $A^{(2)}\in\mathbb{R}^{n_2\times p}$. To get the activities in the next layer, first we must add a row of ones in the same way we did with the input layer. Using this, we can get the collection of activities in the $L$'th layer by:
\begin{equation}
A^{(L)}=g(\Theta^{(L)}A^{(L-1)*})
\end{equation}
Here, the asterisk is a shorthand for adding a row of ones, similarly to equation \ref{row_of_ones}. In general $A^{(L)}\in\mathbb{R}^{n_L\times p}$. Then we may rewrite \ref{hypothesis} simply as:
\begin{equation}
H=A^{(K)}=g\left(\Theta^{(K)}\left(\cdots g(\Theta^{(2)}X)\right)^*\cdots\right)
\end{equation}
Here, $H$ is a $m_K\times p$ matrix containing the hypotheses for each training set data point:
\begin{equation}
H=
\begin{pmatrix}
|	&	\cdots	& |	\\
h_1	&	\cdots	& h_p \\
|	&	\cdots	& |
\end{pmatrix}
\end{equation}
The errors for the last layer is now:
\begin{equation}
\Delta^{(K)}=H-Y
\end{equation}
Here $Y$ is a matrix containing the expected outcomes according to the training set:
\begin{equation}
Y=
\begin{pmatrix}
|	&	\cdots	& |	\\
y_1	&	\cdots	& y_p \\
|	&	\cdots	& |
\end{pmatrix}
\end{equation}
Following the pattern of equation \ref{general_derivative} we calculate:
\begin{equation}
\Delta^{(K)}\left(A^{(K-1)*}\right)^T=\begin{pmatrix}
|	&	\cdots	& |	\\
\delta^{(K)}_1	&	\cdots	& \delta^{(K)}_p \\
|	&	\cdots	& |
\end{pmatrix}
\begin{pmatrix}
1 & -	& a^{(K-1)}_1	&	- \\
\vdots & \vdots	& \vdots	& \vdots \\
1 & -	& a^{(K-1)}_p	&	-
\end{pmatrix}
\end{equation}
This means that the $ij$-element of the product matrix is the sum over the $ij$-derivatives for each of the $p$ data point. It is customary to divide by $p$ to get an average, so that different training set sizes have comparable dynamics during gradual descent.

To backpropagate, we need to calculate the errors in the previous layers. Following equation \ref{backpropagating_errors}, this is done through:
\begin{equation}
\Delta^{(L-1)}=\left(\theta^{(L)}\right)^T\Delta^{(K)}\odot A^{(L-1)}\odot(1-A^{(L-1)})
\end{equation}
Note that the lower case theta is used here, as there's no bias error to propagate back to.

\end{document}