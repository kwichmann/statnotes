\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}

\title{The linear model}
\author{Kristian Wichmann}

\begin{document}

\maketitle

The \textit{linear model} is a theoretical framework that unifies a number of statistical concepts, like ANOVA and regression.

\section{Derivatives and linear algebra}
We will need a few results concerning derivatives of linear algebra expressions. Consider a linear function:
\begin{equation}
f: \mathbb{R}^n\rightarrow\mathbb{R}, f(\beta)=A\beta=
\begin{pmatrix}
a_1 & a_2 & \cdots & a_n
\end{pmatrix}
\begin{pmatrix}
\beta_1 \\ \beta_2 \\ \vdots \\ \beta_n
\end{pmatrix}
\end{equation}
Here, $A\in\mathbb{R}^{1\times n}$ and $\beta\in\mathbb{R}^{n\times 1}$, so in other words:
\begin{equation}
f(\beta)=a_1\beta_1+a_2\beta_2+\cdots a_n\beta_n
\end{equation}
The (multidimensional) derivate is therefore:
\begin{equation}
\label{scalarproductdif}
\frac{\partial f}{\partial \beta}=\nabla_\beta f=
\begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_n
\end{pmatrix}
=A^t
\end{equation}
Similarly, consider a quadratic form in $\beta$:
\begin{equation}
g: \mathbb{R}^n\rightarrow\mathbb{R}, g(\beta)=\beta^t A\beta =
\begin{pmatrix}
\beta_1 & \beta_2 & \cdots & \beta_n
\end{pmatrix}
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\begin{pmatrix}
\beta_1 \\ \beta_2 \\ \vdots \\ \beta_n
\end{pmatrix}
\end{equation}
Here, $A\in\mathbb{R}^{n\times n}$ and $\beta\in\mathbb{R}^{n\times 1}$. Furthermore, $A$ is assumed to be symmetric, such that $a_{ij}=a_{ji}$. Multiplying out, this means that:
\begin{equation}
g(\beta)=\sum_{i=1}^n\sum_{j=1}^n\beta_i a_{ij}\beta_j
\end{equation}
Differentiating with respect to $\beta_k$ only terms where $i=k$ or $i=j$ will contribute. However, the case $i=j=k$ is distinct. So, when $i=k$ we get the contribution $a_{kj}\beta_j$. When $j=k$ we get $\beta_i a_{ij}$. And when $i=j=k$ we get $2a_{kk}\beta_k$. All in all, when summing up, we get two of each $a$-$\beta$ set (because of the symmetry of $A$). So:
\begin{equation}
\frac{\partial g}{\partial\beta_k}=2\sum_{i=1}^n a_{ik}\beta_i
\end{equation}
Or more compactly:
\begin{equation}
\label{quadformdif}
\frac{\partial g}{\partial \beta}=\nabla_\beta g=2A\beta
\end{equation}

\section{Least squares estimation}

\subsection{Statement of the problem}
The general problem is this: We wish to model a linear relationship between a response variables $Y$ and $p$ predictor variables $X_1, X_2,\cdots X_p$. In other words:
\begin{equation}
\label{linear1}
Y=\beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
\end{equation}
Here, the $\beta$'s are the coefficients corresponding to the $X$'s. Now, assume that we have $n$ 'data points', so that $y_i$ corresponds to $x_{i1}, x_{i2},\cdots x_{ip}$. In matrix form equation $(\ref{linear1})$ now becomes: 
\begin{equation}
\label{linear2}
y=X\beta
\end{equation}
Here, $y\in\mathbb{R}^{n\times 1}, X\in\mathbb{R}^{n\times p}$ and $\beta\in\mathbb{R}^{p\times 1}$:
\begin{equation}
y = 
\begin{pmatrix}
	y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},\quad
X =
\begin{pmatrix}
	x_{11} 	& x_{12} 	& \ldots	& x_{1p} \\
	x_{21} 	& x_{22} 	& \ldots	& x_{2p} \\
	\vdots	& \vdots	& \ddots	& \vdots \\
	x_{n1} 	& x_{n2} 	& \ldots	& x_{np} \\
\end{pmatrix},\quad
\beta =
\begin{pmatrix}
	\beta_1	\\ \beta_2	\\ \vdots\\	\beta_p 
\end{pmatrix}
\end{equation}
$X$ is known as the \textit{design matrix}. Given $y$ and $X$, we seek the best fit for $\beta$.

\subsection{Least squares}
There's a number of criteria one could use to pick the best fitting $\beta$. Here, we will search for the one that minimizes the square of the differences in predicted and actual $y$ values. We'll denote this set of parameters as $\hat{\beta}$. The squared difference is:
\begin{align*}
||y-X\hat{\beta}||^2 &=(y-X\hat{\beta})^t(y-X\hat{\beta})=(y^t-\hat{\beta}^t X^t)(y-X\hat{\beta})\\
&=y^t y - 2y^t X\hat{\beta} + \hat{\beta}^t X^t X\hat{\beta}
\end{align*}
Taking the derivative with respect to $\beta$ we can now use equations $(\ref{scalarproductdif})$ and $(\ref{quadformdif})$ to yield:
\begin{equation}
2X^t y - 2X^t X\hat{\beta}
\end{equation}
Since we're looking for a minimum, this vector should be equal to zero:
\begin{equation}
\label{normaleq}
2X^t y - 2X^t X\hat{\beta}=0\Leftrightarrow\hat{\beta}=(X^t X)^{-1}X^t y
\end{equation}
Here it has been assumed that $X^t X$ is invertible. These are known as the \textit{normal equations} for the model. Inserting into equation $(\ref{linear2})$ we get the corresponding predicted $y$-values, also denoted by a hat:
\begin{equation}
\hat{y}=X\hat{\beta}=\underbrace{X(X^t X)^{-1}X^t}_{H}y
\end{equation}
The matrix $H=X(X^t X)^{-1}X^t$ is often called the \textit{hat matrix}, since it puts the hat on the $y$'s. The hat matrix can also be used to find \textit{residuals}, i.e. the difference between actual and predicted $y$-values:
\begin{equation}
e=y-\hat{y}=y-Hy=\underbrace{(I-H)}_{M} y
\end{equation}

\section{Geometric picture}
It is useful to adapt the picture of the columns of $X$ spanning a $p$-dimensional hyperplane in $n$-dimensional space. $y$ is then a vector, and $X\hat{\beta}$ is found by projecting $y$ onto the hyperplane; The corresponding point is exactly the one that minimizes the distance between $y$ (as a point) and the hyperplane.

\subsection{Projection operators}
A linear map that is symmetric and idempotent is called a \textit{projection}. A matrix corresponding to such a mapping is a projection matrix.

\begin{theorem}
The hat matrix $H$ is a projection matrix.
\end{theorem}
\begin{proof}
We need to show that $H$ is symmetric and idempotent. Symmetry:
\begin{equation}
X(X^t X)^{-1}X^t)^t=X\left[(X^t X)^{-1}\right]^t X^t
\end{equation}
But the transpose of an inverse is the same as the inverse of the transpose, so:
\begin{equation}
\left[(X^t X)^{-1}\right]^t=\left[(X^t X)^t\right]^{-1}=(X^t X)^{-1}
\end{equation}
This proves the symmetry of $H$. Idempotency:
\begin{equation}
H^2=\left[X(X^t X)^{-1}X^t\right]^2=X(X^t X)^{-1}X^tX(X^t X)^{-1}X^t=X(X^t X)^{-1}X^t=H
\end{equation}
\end{proof}

This also turns out to be true for the matrix used to find residuals:

\begin{theorem}
The matrix $M=I-H$ is a projection matrix.
\end{theorem}
\begin{proof}
Symmetry follows from the symmetry of $H$. Idempotency:
\begin{equation}
M^2=(I-H)^2=I^2+H^2-2H=I+H-2H=I-H=M
\end{equation}
\end{proof}

\section{The error term}
Obviously, the model described by equation $(\ref{linear2})$ allows for no random variation as written. We need an \textit{error term} to describe the random variation:
\begin{equation}
y=X\beta + \epsilon
\end{equation}
Here $\epsilon$ is a stochastic vector of dimension $n$. The basic assumption of the linear model is that the elements of $\epsilon$ are i.i.d. and normally distributed with mean zero: $\epsilon_i\sim N(0,\sigma^2)$. Or equivalently, that the error term vector follows a multivariate normal distribution: $\epsilon\sim N(0,\sigma^2 I)$. 

\end{document}