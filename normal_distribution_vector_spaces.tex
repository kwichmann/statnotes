\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png}

\title{Normal distributions on vector spaces}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Affine transformations of euclidean spaces}
Let $s:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear transformation. This means that the is an $m\times n$ matrix $A$ so $s(x)=Ax$.

An \textit{affine} transformation $t$ is formed by following this linear map by a translation:
\begin{equation}
t:\mathbb{R}^n\rightarrow\mathbb{R}^m, t(x)=Ax+v
\end{equation}
Here, $v\in\mathbb{R}^m$. Since translations are always bijective, we note that $t$ is bijective iff $A$ is invertible.

Each component of an affine transformation is composed from measurable function - is is understood that we mean with respect to the Borel algebras of each space) - so the affine transformation itself is measurable as well.

\subsection{Transformation properties of the Lebesgue measure}
\label{euclidean_lebesgue_properties}
Recall that the Lebesgue measure in $n$ dimensions $m_n$ is invariant under translation: If $t$ is a translation $t:\mathbb{R}^n\rightarrow\mathbb{R}^n, t(x)=x+x_0$, where $x_0\in\mathbb{R}^n$ then:
\begin{equation}
t(m_n)=m_n
\end{equation}

Also, if $s:\mathbb{R}^n\rightarrow\mathbb{R}^n, x\mapsto Ax$ is an isomorphism, then:
\begin{equation}
s(m_n)=m_n|\det A^{-1}|
\end{equation}
Combining the two, the formula for affine transformation is the same as for linear ones.

\section{Orthogonal complement}
Let $V$ be a finite-dimensional vector space with an inner product $\langle\cdot,\cdot\rangle$. Let $U$ be a subspace of $V$. Then we define the \textit{orthogonal complement} of $U$ as:
\begin{equation}
U^\perp=\{v\in V|\forall u\in U: \langle u, v\rangle = 0\}
\end{equation}
\begin{theorem}
$U^\perp$ is a subspace of $V$.
\end{theorem}
\begin{proof}
According to the subspace theorem, we need to show three things:
\begin{itemize}
\item $U^\perp$ is not empty: Clearly $0\in U^\perp$.
\item Closed under addition: If $v_1,v_2\in U^\perp$, then for all $u\in U^\perp$:
\begin{equation}
\langle v_1+v_2,u\rangle=\langle v_1,u\rangle + \langle v_2,u\rangle = 0 
\end{equation}
\item Closed under scalar multiplication: If $v\in U^\perp$ and $c\in\mathbb{R}$ then for all $u\in U^\perp$:
\begin{equation}
\langle cv,u\rangle = c\langle v,u\rangle = 0
\end{equation}
\end{itemize}
\end{proof}

Since the only vector perpendicular to itself is $0$, we further conclude that $U\cap U^\perp=\{0\}$.

\begin{theorem}
If $e_1, e_2,\ldots,e_m$ is an orthonormal basis for $U$, then for any $v\in V$:
\begin{equation}
v-\sum_{i=1}^m\langle v,e_i\rangle e_i\in U^\perp
\end{equation}
\end{theorem}
\begin{proof}
Let $u\in U$. Then we can write $u=\sum_{j=1}^m\lambda_j e_j$ for some coefficients $\lambda_j$. Now calculate the inner product with the vector above:
\begin{equation}
\langle v-\sum_{i=1}^m\langle v,e_i\rangle e_i\ ,\ \sum_{j=1}^m\lambda_j e_j\rangle=\sum_{i=j}^m\lambda_j\langle v,e_j\rangle-\sum_{i=1}^m\sum_{j=1}^m\lambda_j\langle v,e_i\rangle\langle e_i,e_j\rangle
\end{equation}
Since $\langle e_i,e_j\rangle=\delta_{ij}$ this vanishes.
\end{proof}

This means that we may write any $v\in V$ as a sum of vectors from $U$ and $U^\perp$ respectively:
\begin{equation}
\label{u_uperp}
v=\underbrace{\sum_{i=1}^m\langle v,e_i\rangle e_i}_{\in U}\ +\ \underbrace{v-\sum_{i=1}^m\langle v,e_i\rangle e_i}_{\in U^\perp}
\end{equation}

\begin{theorem}
The decomposition into elements from $U$ and $U^\perp$ from equation \ref{u_uperp} is unique.
\end{theorem}
\begin{proof}
Let $v=u_1+u^\perp_1$ and $v=u_2+u^\perp_2$ be two such decompositions. Then $u_1+u^\perp_1=u_2+u^\perp_2$ and hence $u_1-u_2=u^\perp_2-u^\perp_1$. But this means that this vector is a member of both $U$ and $U^\perp$, and hence it must be $0$. This means $u_1=u_2$ and $u^\perp_1=u^\perp_2$.
\end{proof}

\subsection{The orthogonal projection}
The previous section motivates the following:
\begin{definition}
Let $V$ be a finite-dimensional inner product vector space and $U$ a subspace of $V$. The orthogonal projection from $V$ onto $U$ is the map $p:V\rightarrow V$ which satisfies:
\begin{equation}
\forall v\in V:\quad p(v)\in U,\quad v-p(v)\in U^\perp
\end{equation}
\end{definition}
As we see, one could also define the co-domain of $p$ to be $U$. Usually, the distinction will not matter much.

\begin{theorem}
The orthogonal projection operator is linear.
\end{theorem}
\begin{proof}
We need to show additivity and homogeneity:
\begin{itemize}
\item Additivity: Let $v_1,v_2\in V$. Then $p(v_1)+p(v_2)\in U$ and:
\begin{equation}
v_1-p(v_1)+v_2-p(v_2)=v_1+v_2-(p(v_1)+p(v_2))\in U^\perp
\end{equation}
Adding the two we get $v_1+v_2$. So $p(v_1+v_2)=p(v_1)+p(v_2)$.
\item Homogeneity. Let $v\in V$ and $c\in\mathbb{R}$. Then $cp(v)\in U$ and $c(v-p(v))=cv-cp(v)\in U^\perp$. Adding the two we get $cv$, so $p(cv)=cp(v)$. 
\end{itemize}
\end{proof}

\begin{theorem}
The orthogonal projection operator $p:V\rightarrow V$ is idempotent. I.e. $p\circ p=p$.
\end{theorem}
\begin{proof}
Let $v\in V$. Then $p(v)\in U$. But this means that the decomposition of $p(v)$ is $p(v)+0$. So $p\circ p(v)=p(v)$.
\end{proof}

\section{Lebesgue measures on vector spaces}

\subsection{Coordinate maps}
Let $V$ be a finite-dimensional vector space of dimension $n$. Our question is, if we can turn $V$ into a measure space in a natural way. Since we know that $V$ is isomorphic to $\mathbb{R}^n$, it makes sense to tweak the usual Lebesgue measure in $N$ dimensions:

Let $e_1,e_2,\ldots,e_n$ be a basis for $V$. Then we can define the \textit{coordinate map} as follows:
\begin{equation}
\phi:\mathbb{R}^n\rightarrow V,\quad
\begin{pmatrix}
x_1	\\	x_2	\\ \vdots	\\ x_n
\end{pmatrix}
\mapsto\sum_{i=1}^n x_i e_i
\end{equation}
This is obviously an isomorphism. Specifically, it is invertible with inverse $\phi^{-1}: V\rightarrow\mathbb{R}^n$.

The coordinate map depends on the chosen basis. If we had chosen another basis $e^*_1,e^*_2,\ldots,e^*_n$ we would get another isomorphism $\phi^*$.

\subsection{Borel algebra on $V$}
We can now use $\phi^{-1}$ to induce a $\sigma$-algebra on $V$. Set $\mathbb{B}_V$ to the smallest $\sigma$-algebra that makes $\phi^{-1}$ measurable when $\mathbb{R}^n$ is equipped with the Borel algebra $\mathbb{B}_n$. We call $\mathbb{B}_V$ the \textit{Borel algebra on $V$}.

At first this object seems to depend of the choice of basis for $V$. But it turns out that the use of definite article in the definition is justified:
\begin{theorem}
If $e_1,e_2,\ldots,e_n$ and $e^*_1,e^*_2,\ldots,e^*_n$ are bases for $V$, then the induced $\sigma$-algebra $\mathbb{B}_V$ and $\mathbb{B}^*_V$ is the same thing.
\end{theorem}
\begin{proof}
We know that $\phi^{-1}$ is $\mathbb{B}_V-\mathbb{B}_n$ measurable by definition. We have:
\begin{equation}
(\phi^*)^{-1}=(\phi^*)^{-1}\circ\textrm{id}_V=(\phi^*)^{-1}\circ(\phi\circ\phi^{-1})=\left((\phi^*)^{-1}\circ\phi\right)\circ\phi^{-1}
\end{equation}
$\left((\phi^*)^{-1}\circ\phi\right)$ is a linear operator on $\mathbb{R}^n$ and so according to section \ref{euclidean_lebesgue_properties} is measurable. So $(\phi^*)^{-1}$ must be $\mathbb{B}_V-\mathbb{B}_n$-measurable. Since $\mathbb{B}_V^*$ is the smallest $\sigma$-algebra to make $(\phi^*)^{-1}$ $\mathbb{B}_V-\mathbb{B}_n$-measurable, we must have $\mathbb{B}^*_V\subseteq\mathbb{B}_V$.

But by a totally symmetric argument, we must also have $\mathbb{B}_V\subseteq\mathbb{B}^*_V$. Hence $\mathbb{B}_V=\mathbb{B}^*_V$.
\end{proof}

It turns out, that $\phi$ must be measurable too. This is a direct consequence of the pipeline lemma.

\begin{theorem}
Given two finite-dimensional vector spaces $V$ and $W$, then:
\begin{equation}
\mathbb{B}_{V\times W}=\mathbb{B}_V\otimes\mathbb{B}_W
\end{equation}
\end{theorem}
\begin{proof}
Let $e_1, e_2,\ldots, e_n$ be a basis for $V$ with corresponding coordinate map $\phi$. And $f_1, f_2,\ldots, f_m$ a basis for $W$ with corresponding coordinate map $\psi$. Then $(e_1, 0), (e_2, 0),\ldots,(e_n,0),(0, f_1), (0, f_2),\ldots,(0,f_m)$ is a basis for $V\times W$. The corresponding coordinate map is:
\begin{equation}
\phi\times\psi: (x_1, x_2,\ldots,x_{n+m})\mapsto\left(\sum_{i=1}^n x_i e_i,\sum_{j=1}^m x_{n+j}f_j\right)
\end{equation}
The inverse is $(\phi\times\psi)^{-1}=\phi^{-1}\times\psi^{-1}$. Since $\phi^{-1}$ is $\mathbb{B}_V-\mathbb{B}_n$-measurable and $\psi^{-1}$ is $\mathbb{B}_W-\mathbb{B}_m$-measurable, $\phi^{-1}\times\psi^{-1}$ must be $\mathbb{B}_V\otimes\mathbb{B}_W-\mathbb{B}_n\otimes\mathbb{B}_n$-measurable. But $\mathbb{B}_n\otimes\mathbb{B}_m=\mathbb{B}_{n+m}$. Since $\mathbb{B}_{V\times W}$ is the smallest $\sigma$-algebra to make $\phi^{-1}\times\psi^{-1}$ measurable, we must have $\mathbb{B}_{V\times W}\subseteq\mathbb{B}_V\otimes\mathbb{B}_W$.

On the other hand, consider the projection operators:
\begin{align}
\pi_V:& V\times W\rightarrow V, (v,w)\mapsto v\\
\pi_n:& \mathbb{R}^{n+m}\rightarrow\mathbb{R}^n, (x_1,\ldots,x_{n+m})\mapsto(x_1,\ldots,x_n)
\end{align}
Now consider $\pi_V\circ(\phi\times\psi)$. Applied to an $x\in\mathbb{R}^{n+m}$ we have:
\begin{equation}
\pi_V\circ(\phi\times\psi)(x)=\pi_v\left(\left(\sum_{i=1}^n x_i e_i,\sum_{j=1}^m x_{n+j}f_j\right)\right)=\sum_{i=1}^n x_i e_i
\end{equation}
But this is the same as:
\begin{equation}
\phi\circ\pi_1(x)=\phi\left((x_1,\ldots x_n)\right)=\sum_{i=1}^n x_i e_i
\end{equation}
So $\pi_V\circ(\phi\times\psi)=\phi\circ\pi_1$. Now apply $\phi^{-1}\times\psi^{-1}$ from the right to get:
\begin{equation}
\pi_V=\phi\circ\pi_1\circ(\phi^{-1}\times\psi^{-1})
\end{equation}
Since all the three functions on the right side are measurable, $\pi_V$ must be $\mathbb{B}_{V\times W}-\mathbb{B}_V$-measurable. By a similar argument the corresponding projection operator $\pi_W: V\times W\rightarrow W$ is $\mathbb{B}_{V\times W}-\mathbb{B}_W$-measurable. Since $\mathbb{B}_V\otimes\mathbb{B}_W$ is the smallest $\sigma$-algebra to make both $\pi_V$ and $\pi_W$ measurable, we must have: $\mathbb{B}_V\otimes\mathbb{B}_W\subseteq\mathbb{B}_{V\times W}$.
\end{proof}

\subsection{Lebesgue measures on $V$}
We now want to define a measure on the measurable space $(V,\mathbb{B}_V)$. If $e_1, e_2,\ldots, e_n$ is a basis for $V$, we will use the associated coordinate map $\phi$ to define a measure:
\begin{equation}
\lambda_V=\phi(m_n)
\end{equation}
Here, $m_n$ is the usual Lebesgue measure in $n$ dimensions. The problem is, that this measure depends on the chosen basis! Consider another basis $e^*_1, e^*_2,\ldots,e^*_n$ and associated coordinate map $\phi^*$. Then the measure is:
\begin{equation}
\lambda^*_V=\phi^*(m_n)=(\phi\circ\phi^{-1})\circ\phi^*(m_n)=\phi\circ(\phi^{-1}\circ\phi^*(m_n))
\end{equation}
Now $\phi^{-1}\circ\phi^*$ is an isomorphism $\mathbb{R}^n\rightarrow\mathbb{R}^n$, so according to section \ref{euclidean_lebesgue_properties}, there is a constant $c$ such that $(\phi^{-1}\circ\phi^*(m_n))=c m_n$. So:
\begin{equation}
\lambda^*_V=c\phi(m_n)=c\lambda_V
\end{equation}

So while there are many Lebesgue measures on $V$ they only differ from each other by a constant factor. This means that they all agree on what constitutes a null set, and on which functions are integrable. They disagree on the integral, but agree on whether it is finite or not. The also agree on whether a measure $\mu$ has a density with respect to $\lambda_V$ or not.



\section{Random vectors}
In this section, we consider vectors of random variables. So a random vector of dimension $n$ is:
\begin{equation}
X=
\begin{pmatrix}
X_1 \\ X_2 \\ \vdots \\ X_n
\end{pmatrix}
\end{equation}
Here, each $X_i$ is a random variable.

\subsection{Variance}
The variance of a $n$-dimensional vector is the $n\times n$ matrix:
\begin{equation}
\textrm{Var}(X)=E[(X-\mu_X)(X-\mu_X)^t]
\end{equation}
Here $\mu_X=E[X]$, i.e. the vector of expectation values of the $X_i$'s. From the usual definitions of variances and covariances between random variables, we see that the diagonal of $\textrm{Var}(X)$ contains the variances of each $X_i$, while the off diagonal elements are the covariances between variables:
\begin{equation}
[\textrm{Var}(X)]_{ij}=\textrm{Cov}(X_i,X_j)
\end{equation}
Due to the symmetry of covariance, this means that $\textrm{Var}(X)$ is a symmetric matrix.

\subsubsection{Variance calculation rules}
Similarly to ordinary random variables, we might calculate the variance matrix as follows:
\begin{align}
\textrm{Var}(X)=&E[(X-\mu_X)(X-\mu_X)^t]=\\
&E(XX^t)-E(X)\mu_X^t-\mu_X E(X)^t+\mu_X\mu_X^t=\\
&E(XX^t)-\mu_X\mu_X^t
\end{align}
Here, we've used the linearity of the expectation value and the definition of $\mu_X$.

Adding a constant vector $b$ does not change the variance, since $E[X+b]=\mu_X+b$:
\begin{equation}
\textrm{Var}(X+b)=E[(X+b-(\mu_X+b))(X+b-(\mu_X+b))^t]=E[(X-\mu_X)(X-\mu_X)^t]
\end{equation}
This is just the variance of $X$.

If $A$ is a constant $m\times n$ matrix and $X$ is an $n$-dimensional random vector, then:
\begin{align}
\textrm{Var}(AX)=&E[(AX-A\mu_X)(AX-A\mu_X)^t]=\\
&E[(A(X-\mu_X))(A(X-\mu_X))^t]=\\
&E[A(X-\mu_X)(X-\mu_X)^t A^t]=\\
&A[(X-\mu_X)(X-\mu_X)^t]A^t
\end{align}
So we have $\textrm{Var}(AX)=A\ \textrm{Var}(X)A^t$.

\subsection{Covariance}
The covariance matrix between two variable vectors $X$ and $Y$ is:
\begin{equation}
\textrm{Cov}(X,Y)=E[(X-\mu_X)(Y-\mu_Y)^t]
\end{equation}
If $X$ has dimension $m$ and $Y$ dimension $n$, then $\textrm{Cov}(X,Y)$ has dimension $m\times n$. Here, the matrix elements reduce to ordinary covariances between $X_i$'s and $Y_j$s:
\begin{equation}
[\textrm{Cov}(X,Y)]_{ij}=\textrm{Cov}(X_i,Y_j)
\end{equation}
We note, that the variance could have been defined as a special case of covariance, since $\textrm{Var}(X)=\textrm{Cov}(X,X)$.

\subsubsection{Covariance calculation rules}
Similarly to the rule for variances, we have:
\begin{equation}
\textrm{Cov}(X,Y)=E[XY^t]-\mu_X\mu_Y^t
\end{equation}
The proof is essentially the same.

If $A$ and $B$ are constant matrices of appropriate dimesion, we also have:
\begin{equation}
\textrm{Cov}(AX,BY)=A\ \textrm{Cov}(X,Y)B^t
\end{equation}
Again, the proof is entirely analogous to the corresponding variance formula.

The covariance is bilinear:
\begin{align}
\textrm{Cov}(X+Y,Z)=&\textrm{Cov}(X,Z)+\textrm{Cov}(Y,Z)\\
\textrm{Cov}(X,Y+Z)=&\textrm{Cov}(X,Y)+\textrm{Cov}(X,Z)
\end{align}
This follows from the bilinearity of the ordinary covariance.

\subsubsection{Addiational variance formulas}
Since we noted that $\textrm{Var}(X)=\textrm{Cov}(X,X)$, we may use these rules to derive further properties of variances.

For instance, the variance of a sum:
\begin{align}
\textrm{Var}(X+Y)=&\textrm{Cov}(X+Y,X+Y)=\\
&\textrm{Cov}(X,X)+\textrm{Cov}(X,Y)+\textrm{Cov}(Y,X)+\textrm{Cov}(Y,Y)=\\
&\textrm{Var}(X)+\textrm{Var}(Y)+\textrm{Cov}(X,Y)+\textrm{Cov}(Y,X)
\end{align}
This mirrors the formula for the covariance of sums ordinary random variables, but is complicated by the fact that the vector covariance is not symmetric.

\subsection{Quadratic forms}
If $X$ is an $n$-dimensional random variable and $A$ an $n\times n$ matrix, then the corresponding quadratic form is $Q=X^tAX$. I.e. a scalar. What is the expectation value of the quadratic form? We can use a trick here. Since $Q$ is a scalar, we can trivially write this as a trace:
\begin{equation}
Q=X^tAX=\textrm{tr}(X^tAX)=\textrm{tr}(AXX^t)
\end{equation}
Here, we've used the cyclic property of traces. Now, the expectation value is:
\begin{equation}
E[Q]=E[\textrm{tr}(AXX^t)]=\textrm{tr}(E[AXX^t])=\textrm{tr}(A\ E[XX^t])
\end{equation}
But we know, that $\textrm{Var}(X)=E(XX^t)-\mu_X\mu_X^t$, so $E[XX^t]=\textrm{Var}(X)+\mu_X\mu_X^t$:
\begin{equation}
E[Q]=\textrm{tr}(A(\textrm{Var}(X)+\mu_X\mu_X^t))=\textrm{tr}(A\ \textrm{Var}(X))+\textrm{tr}(A\mu_X\mu_X^t)
\end{equation}
The last term may be rewritten:
\begin{equation}
\textrm{tr}(A\mu_X\mu_X^t)=\textrm{tr}(\mu_X^t A\mu_X)=\mu_X^t A\mu_X
\end{equation}
In the last step we've used that the contents of the parenthesis is a scalar. So all in all:
\begin{equation}
E[X^tAX]=\textrm{tr}(A\ \textrm{Var}(X))+\mu_X^t A\mu_X
\end{equation}

\end{document}