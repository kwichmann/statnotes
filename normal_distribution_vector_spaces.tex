\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png}

\title{Normal distributions on vector spaces}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Univariate normal distributions}
The \textit{standard normal distribution} is given by the density function:
\begin{equation}
\label{uni_snd}
\phi(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
\end{equation}
This, like all density functions in this section, is understood to be with respect to the Lebesgue integral on $\mathbb{R}$. A random variable $Z$ that follows this distribution is said to be standard normally distributed, and we write $Z\sim N(0,1)$. It can be shown that $E[Z]=0$ and $\textrm{Var}(Z)=1$.

A general, univariate normal distribution with parameters $\mu$ and $\sigma$, is given by the distribution of $X=\mu+\sigma Z$. If $\sigma\neq 0$, we can invert to find $z=\frac{x-\mu}{\sigma}$. So $\frac{dz}{dx}=\frac{1}{\sigma}$. According to the usual transformation rules, $x$ has the density function:
\begin{equation}
f(x)=\phi\left(\frac{x-\mu}{\sigma}\right)\frac{1}{\sigma}=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}
\end{equation}
We write $X\sim N(\mu,\sigma^2)$. From the usual rules of expectation values and variances, it follows that $E[X]=\mu$ and $\textrm{Var}(X)=\sigma^2$.

\subsection{What if $\sigma=0$?}
The derivation above assumes $\sigma$ to be non-zero. But even if this is the case, $X=\mu$ still has a distribution - it is simply $\mu$ all the time. However, this random variable does not have a density function with respect to the Lebesgue measure. In light of the Radon-Nikodym theorem, this is because the Lebesgue measure does not dominate the probability measure of $X$: $P_X(\{\mu\})=1, m_1(\{\mu\})=0$.

In the multivariate case, we will often run into similar problems.

\section{Random vectors}
Before tackling the multivariate case, we need some basic tools. In this section, we consider vectors of random variables. So a random vector of dimension $n$ is:
\begin{equation}
X=
\begin{pmatrix}
X_1 \\ X_2 \\ \vdots \\ X_n
\end{pmatrix}
\end{equation}
Here, each $X_i$ is a random variable.

\subsection{Variance}
The variance of a $n$-dimensional vector is the $n\times n$ matrix:
\begin{equation}
\textrm{Var}(X)=E[(X-\mu_X)(X-\mu_X)^t]
\end{equation}
Here $\mu_X=E[X]$, i.e. the vector of expectation values of the $X_i$'s. From the usual definitions of variances and covariances between random variables, we see that the diagonal of $\textrm{Var}(X)$ contains the variances of each $X_i$, while the off diagonal elements are the covariances between variables:
\begin{equation}
[\textrm{Var}(X)]_{ij}=\textrm{Cov}(X_i,X_j)
\end{equation}
Due to the symmetry of covariance, this means that $\textrm{Var}(X)$ is a symmetric matrix.

\subsubsection{Variance calculation rules}
Similarly to ordinary random variables, we might calculate the variance matrix as follows:
\begin{align}
\textrm{Var}(X)=&E[(X-\mu_X)(X-\mu_X)^t]=\\
&E(XX^t)-E(X)\mu_X^t-\mu_X E(X)^t+\mu_X\mu_X^t=\\
&E(XX^t)-\mu_X\mu_X^t
\end{align}
Here, we've used the linearity of the expectation value and the definition of $\mu_X$.

Adding a constant vector $b$ does not change the variance, since $E[X+b]=\mu_X+b$:
\begin{equation}
\textrm{Var}(X+b)=E[(X+b-(\mu_X+b))(X+b-(\mu_X+b))^t]=E[(X-\mu_X)(X-\mu_X)^t]
\end{equation}
This is just the variance of $X$.

If $A$ is a constant $m\times n$ matrix and $X$ is an $n$-dimensional random vector, then:
\begin{align}
\textrm{Var}(AX)=&E[(AX-A\mu_X)(AX-A\mu_X)^t]=\\
&E[(A(X-\mu_X))(A(X-\mu_X))^t]=\\
&E[A(X-\mu_X)(X-\mu_X)^t A^t]=\\
&A[(X-\mu_X)(X-\mu_X)^t]A^t
\end{align}
So we have $\textrm{Var}(AX)=A\ \textrm{Var}(X)A^t$.

\subsection{Covariance}
The covariance matrix between two variable vectors $X$ and $Y$ is:
\begin{equation}
\textrm{Cov}(X,Y)=E[(X-\mu_X)(Y-\mu_Y)^t]
\end{equation}
If $X$ has dimension $m$ and $Y$ dimension $n$, then $\textrm{Cov}(X,Y)$ has dimension $m\times n$. Here, the matrix elements reduce to ordinary covariances between $X_i$'s and $Y_j$s:
\begin{equation}
[\textrm{Cov}(X,Y)]_{ij}=\textrm{Cov}(X_i,Y_j)
\end{equation}
This also means, that $\textrm{Cov}(X,Y)=\left(\textrm{Cov}(Y,X)\right)^t$

We note, that the variance could have been defined as a special case of covariance, since $\textrm{Var}(X)=\textrm{Cov}(X,X)$.

\subsubsection{Covariance calculation rules}
Similarly to the rule for variances, we have:
\begin{equation}
\textrm{Cov}(X,Y)=E[XY^t]-\mu_X\mu_Y^t
\end{equation}
The proof is essentially the same.

If $A$ and $B$ are constant matrices of appropriate dimesion, we also have:
\begin{equation}
\textrm{Cov}(AX,BY)=A\ \textrm{Cov}(X,Y)B^t
\end{equation}
Again, the proof is entirely analogous to the corresponding variance formula.

The covariance is bilinear:
\begin{align}
\textrm{Cov}(X+Y,Z)=&\textrm{Cov}(X,Z)+\textrm{Cov}(Y,Z)\\
\textrm{Cov}(X,Y+Z)=&\textrm{Cov}(X,Y)+\textrm{Cov}(X,Z)
\end{align}
This follows from the bilinearity of the ordinary covariance.

\subsubsection{Addiational variance formulas}
Since we noted that $\textrm{Var}(X)=\textrm{Cov}(X,X)$, we may use these rules to derive further properties of variances.

For instance, the variance of a sum:
\begin{align}
\textrm{Var}(X+Y)=&\textrm{Cov}(X+Y,X+Y)=\\
&\textrm{Cov}(X,X)+\textrm{Cov}(X,Y)+\textrm{Cov}(Y,X)+\textrm{Cov}(Y,Y)=\\
&\textrm{Var}(X)+\textrm{Var}(Y)+\textrm{Cov}(X,Y)+\textrm{Cov}(Y,X)
\end{align}
This mirrors the formula for the covariance of sums ordinary random variables, but is complicated by the fact that the vector covariance is not symmetric.

\subsection{Quadratic forms}
If $X$ is an $n$-dimensional random variable and $A$ an $n\times n$ matrix, then the corresponding quadratic form is $Q=X^tAX$. I.e. a scalar. What is the expectation value of the quadratic form? We can use a trick here. Since $Q$ is a scalar, we can trivially write this as a trace:
\begin{equation}
Q=X^tAX=\textrm{tr}(X^tAX)=\textrm{tr}(AXX^t)
\end{equation}
Here, we've used the cyclic property of traces. Now, the expectation value is:
\begin{equation}
E[Q]=E[\textrm{tr}(AXX^t)]=\textrm{tr}(E[AXX^t])=\textrm{tr}(A\ E[XX^t])
\end{equation}
But we know, that $\textrm{Var}(X)=E(XX^t)-\mu_X\mu_X^t$, so $E[XX^t]=\textrm{Var}(X)+\mu_X\mu_X^t$:
\begin{equation}
E[Q]=\textrm{tr}(A(\textrm{Var}(X)+\mu_X\mu_X^t))=\textrm{tr}(A\ \textrm{Var}(X))+\textrm{tr}(A\mu_X\mu_X^t)
\end{equation}
The last term may be rewritten:
\begin{equation}
\textrm{tr}(A\mu_X\mu_X^t)=\textrm{tr}(\mu_X^t A\mu_X)=\mu_X^t A\mu_X
\end{equation}
In the last step we've used that the contents of the parenthesis is a scalar. So all in all:
\begin{equation}
E[X^tAX]=\textrm{tr}(A\ \textrm{Var}(X))+\mu_X^t A\mu_X
\end{equation}

\section{Multivariate normal distributions}
A \textit{multivariate normal distribution} in $n$ dimensions is a random vector $X$ that has the property, that any linear combination of its elements is a univariate normal distribution. In other words $q^t X$ should be a univariate normal for all $q\in\mathbb{R}^n$.

Specifically note, that it is not enough to require each component of the random vector to be normally distributed. This leaves room for some pathologically distributed overall distributions. However, if each of the components are also independent, $X$ is a multivariate normal. To see this, assume that $X_i$ is normally distributed with parameters $\mu_i$ and $\sigma_i^2$. We can now use the usual properties of normals: First we see that $q_i X_i\sim N(q_i\mu_i,q_i^2\sigma_i^2)$. And because of independence we further have:
\begin{equation}
q^t X=q_1 X_1+q_2 X_2+\ldots +q_n X_n\sim N\left(q^t\mu,\sum_{i=1}^n q_i^2\sigma_i^2\right)
\end{equation}

\subsection{Dependence vs. correlation}
Further than that, it turn out that for components of a multivariate normal, independence and uncorrelation is equivalent:
\begin{theorem}
Let $X$ be a multivariate normally distributed and $X_1$ and $X_2$ be components of $X$. Then $X_1$ and $X_2$ are independent if and only if they're uncorrelated.
\end{theorem}
\begin{proof}
The 'only if' part is true for any distributions. It's the other way that's interesting. Assume therefore, that $X_1$ and $X_2$ are uncorrelated. Recall, that the moment-generating function (MGF) of a unidimensional normal distribution $Y$ is given by:
\begin{equation}
\label{mgf_univariate}
M_Y(t)=E\left[e^{tY}\right]=\exp\left(E[tY]+\frac{1}{2}\textrm{Var}(tY)\right)
\end{equation}
Consider now the bivariate distribution:
\begin{equation}
X'=
\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
\end{equation}
The MGF for $X'$ is:
\begin{equation}
M_{X'}(t)=E\left[e^{t^t X'}\right]
\end{equation}
Here, $t$ is two-dimensional as well, and $tY$ from before has been replaced by a dot product. But this is equal to:
\begin{equation}
E\left[\exp(\underbrace{t_1 X_1 + t_2 X_2}_{'tY'})\right]
\end{equation}
Now, by the assumption that $X$ is multivariate normal, the underbraced part must be univariate normal. So we can use (the latter half of) equation \ref{mgf_univariate} to get:
\begin{equation}
E\left[e^{t^t X'}\right]=\exp\left(E[X']t+\frac{1}{2}\textrm{Var}X'\right)
\end{equation}
Now, $E[X']=t_1\mu_1+t_2\mu_2$, where $\mu_1$ and $\mu_2$ are the means of $X_1$ and $X_2$ respectively. And because the two are uncorrelated, $\textrm{Var}X'=t_1^2\sigma_1^2+t_2^2\sigma_2^2$. Similarly, here the sigmas are the standard deviations of the $X$ components. So:
\begin{equation}
M_{X'}(t)=\exp\left[t_1\mu_1+t_2\mu_2+\frac{1}{2}\left(t_1^2\sigma_1^2+t_2^2\sigma_2^2\right)\right]
\end{equation}
But this can be rewritten:
\begin{equation}
\exp\left(t_1\mu_1+\frac{1}{2}t_1^2\sigma_1^2\right)\exp\left(t_2\mu_1+\frac{1}{2}t_2^2\sigma_2^2\right)=M_{X_1}(t_1)M_{X_2}(t_2)
\end{equation}
Since the joint MGF is equal to the product on the individual ones, we conclude that $X_1$ and $X_2$ are independent.
\end{proof}

\subsection{The multivariate standard normal}
The \textit{multivariate standard normal distribution} is the random vector $Z$ that consists of $n$ independent components, all univariate standard normally distributed. According to the previous section, this is a multivariate standard normal. Because of independence, the density function in $\mathbb{R}^n$ is simply a product of terms like equation \ref{uni_snd}:
\begin{equation}
\phi(z)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}}e^{-z_i^2/2}=(2\pi)^{-n/2}e^{-||z||^2/2}
\end{equation}
Here $z\in\mathbb{R}^n$, and $||\cdot ||$ is the standard Euclidean norm. If a $n$-dimensional dimensional random vector $Z$ follows this distribution we write $Z\sim N(0,I_n)$, where $I_n$ is the identity matrix in $n$ dimensions. The reason for this is, that the variance matrix of $X$ is equal to $I_n$.

\subsection{Regular and singular distributions}
If a multivariate normal $X$ has a non-singular variance matrix, we call the distribution \textit{regular}. Is this is not the case, the distribution is called \textit{singular}.

\section{Affine transformations of euclidean spaces}
In order to get to the general, multidimensional normal distribution, we need yet another component:

Let $s:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear transformation. This means that the is an $m\times n$ matrix $A$ so $s(x)=Ax$.

An \textit{affine} transformation $t$ is formed by following this linear map by a translation:
\begin{equation}
t:\mathbb{R}^n\rightarrow\mathbb{R}^m, t(x)=Ax+v
\end{equation}
Here, $v\in\mathbb{R}^m$. Since translations are always bijective, we note that $t$ is bijective iff $A$ is invertible.

Each component of an affine transformation is composed from measurable function - is is understood that we mean with respect to the Borel algebras of each space) - so the affine transformation itself is measurable as well.

\subsection{Transformation properties of the Lebesgue measure}
\label{euclidean_lebesgue_properties}
Recall that the Lebesgue measure in $n$ dimensions $m_n$ is invariant under translation: If $t$ is a translation $t:\mathbb{R}^n\rightarrow\mathbb{R}^n, t(x)=x+x_0$, where $x_0\in\mathbb{R}^n$ then:
\begin{equation}
t(m_n)=m_n
\end{equation}

Also, if $s:\mathbb{R}^n\rightarrow\mathbb{R}^n, x\mapsto Ax$ is an isomorphism, then:
\begin{equation}
\label{lebesgue_transform}
s(m_n)=m_n|\det A^{-1}|
\end{equation}
Combining the two, the formula for affine transformation is the same as for linear ones.

\section{Affine transformations of multivariate normal distributions}

Let's start by considering the following basic fact:
\begin{theorem}
Let $X$ be a multivariate normal distribution in $n$ dimensions. Consider an affine transformation $Y=AX+b$, where $A\in\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^m$. Then $Y$ is a multivariate normal distribution in $m$ dimensions.
\end{theorem}
\begin{proof}
Consider a linear combination of components of $Y$:
\begin{equation}
q^t Y=q^t(AX+b)=\sum_{i,j=1}^m q_i\left(A_{ij}X_j+b_i\right)=\underbrace{\sum_{i,j=1}^m q_i A_{ij}X_j}+q^t b
\end{equation}
The underbraced part is a linear combination of components of $X$ and hence univariate normal. Adding the constant term $q^t b$ doesn't change this fact. Hence, $Y$ is a multivariate normal.
\end{proof}

Using our knowledge of random vectors we can see how such an affine transformation changes the mean and variance:

\begin{theorem}
If $X$ is a random vector with mean $\mu$ and variance $\Sigma$, then the mean and variance of $Y=AX+b$ are $A\mu+b$ and $A\Sigma A^t$ respectively.
\end{theorem}
\begin{proof}
We need to prove both:
\begin{itemize}
\item Mean: $E[Y]=E[AX+b]=E[AX]+b=A E[X]+b=A\mu+b$.
\item Variance: $\textrm{Var}(Y)=\textrm{Var}(AX+b)=\textrm{Var}(AX)=A\textrm{Var}(X)A^t=A\Sigma A^t$.
\end{itemize}
\end{proof}

How does such a transformation affect the regularity of the distribution?

\begin{theorem}
\label{surjective_A}
Assume $X$ is an $n$-dimensional regular normal distribution. If $A\in\mathbb{R}^{m\times n}$ has rank $m$, then $Y=AX+b$ is also regular. Conversely, if $Y$ is regular, then $A$ has rank $m$.
\end{theorem}
\begin{proof}
Since $b$ has no bearing on the variance, we need to show that the variance matrix of $AX$ is invertible. The variance of $AX$ is equal to $A\Sigma A^t$, where $\Sigma$ is the variance of $X$. Since both $A$ and $\Sigma$ have full rank, so does $A\Sigma$. So the total rank is the rank of $A^t$, which is $m$.

Conversely, assume $Y$ to be regular, i.e. $A\Sigma A^t$ has rank $m$. But, the rank of a product is greater than or equal to the minimum rank of the factors. So $\min\{\textrm{rank}A,\textrm{rank}\Sigma\}\ge m$. But then $\textrm{rank}A\ge m$, which is only possible when the equality holds. 
\end{proof}

\subsection{Affine transformation of standard univariate normal}
Given an $n$-dimension random vector $Z\sim N(0,I_n)$, we consider the random variable $X=\mu+AZ$, where $A\in\mathbb{R}^{m\times n}$. From the previous section, we now know that $X$ is a multivariate normal with:
\begin{equation}
E(X)=\mu,\quad \textrm{Var}(X)=AA^t=\Sigma
\end{equation}
Here, as usual $\Sigma$ is the variance matrix of $X$\footnote{Since $\Sigma$ corresponds to $\sigma^2$ in the univariate case, $\Sigma^2$ would in some sense have been a more logical naming choice, but such is tradition.}.

$A$ plays the roles that $\sigma$ played for the univariate case, and hence we might expect that special care needs to be taken when it is "zero", which will turn out to mean "non-surjective" in the multivariate case in accordance with theorem \ref{surjective_A}.

\subsubsection{Invertible $A$}
So we need for $A$ to be surjective in order to get a density function with respect to the Lebesgue measure. For now, let's assume that $m=n$, and that $A$ is also injective and therefore invertible. Then we can solve for $Z$:
\begin{equation}
Z=A^{-1}(X-\mu)
\end{equation}
We may now write
\begin{equation}
||z||^2=z^t z=\left(A^{-1}(x-\mu)\right)^t\left(A^{-1}(x-\mu)\right)=(x-\mu)^t\left(A^{-1}\right)^t A^{-1}(x-\mu)
\end{equation}
According to equation \ref{lebesgue_transform} the density function for $X$ is:
\begin{equation}
f(x)=(2\pi)^{-n/2}\textrm{det}(A^{-1})\exp\left[-\frac{1}{2}(x-\mu)^t\left(A^{-1}\right)^t A^{-1}(x-\mu)\right]
\end{equation}
But the transpose of an inverse is the inverse of a transpose:
\begin{equation}
\left(A^{-1}\right)^t A^{-1}=(A^t)^{-1}A^{-1}=(AA^t)^{-1}=\Sigma^{-1}
\end{equation}
So the density function is:
\begin{equation}
f(x)=(2\pi)^{-n/2}\textrm{det}(A^{-1})\exp\left[-\frac{1}{2}(x-\mu)^t\Sigma^{-1}(x-\mu)\right]
\end{equation}
We may express this solely in terms of $\Sigma$, since $\det\Sigma=\det AA^t=(\det A)^2$. So $\det(A^{-1})=(\det\Sigma)^{-\frac{1}{2}}$. One handy way to express this is:
\begin{equation}
\label{mvn_density}
f(x)=\frac{1}{\sqrt{\det(2\pi\Sigma)}}\exp\left[-\frac{1}{2}(x-\mu)^t\Sigma^{-1}(x-\mu)\right]
\end{equation}
This closely mirrors the univariate formula.

\subsubsection{Non-invertible $A$}
If $A$ is not surjective, $X$ does not have a density with respect to the Lebesgue measure in $\mathbb{R}^m$. This is because the rank of $A$ is less than $m$, and so $X$ only takes on values in an affine, proper subspace of $\mathbb{R}^m$. The $m$-dimensional Lebesgue measure of such a space is zero.

\subsubsection{Single value decomposition}
However, $A$ may still be surjective but not invertible. How to calculate the density function in this case? To do so, it is useful to rewrite $A$ using \textit{single value decomposition}. I.e. if $A$ is a $m\times n$ matrix, then there exists orthogonal matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ such that:
\begin{equation}
A=UDV^t
\end{equation}
Here $D$ is a diagonal (but generally not square) matrix with non-negative diagonal entries, the eigenvalues of $AA^t$. These are known as the \textit{singular values} of $A$. The number of non-zero singular values is equal to the rank of $A$.

\subsection{$A$ from variance $\Sigma$}
Often, we want to specify a univariate normal from a variance $\Sigma$ instead of a transformation matrix $A$. As we saw above, we only need $\Sigma$ to write down the density (if it exists), but what if we want $A$?

To do this, we note that $\Sigma$ is symmetric and positive semi-definite because it is a covariance matrix. So there is an orthogonal matrix $O$ such that $\Sigma=ODO^t$, where $D$ is a diagonal matrix with non-negative diagonal entries. Hence, we can construct another diagonal matrix $D^{\frac{1}{2}}$ where the entries are the square roots of the ones in $D$. Now we obviously have:
\begin{equation}
\Sigma=ODO^t=\underbrace{OD^{\frac{1}{2}}}_{A}\underbrace{D^{\frac{1}{2}}O^t}_{A^t}
\end{equation}
By setting $A=OD^{\frac{1}{2}}$ we have achieved a decomposition of $\Sigma$ that will bring about all the results above (though this is not necessarily the only one). A is sometimes written as the "square root of $\Sigma$": $A=\Sigma^{\frac{1}{2}}$.

\subsubsection{Sphering}
One use of the decomposition $A=OD^{\frac{1}{2}}$ is to gain geometrical insight into multivariate Gaussians by what is called \textit{sphering}.

How can we describe the linear map $A$? It is a composite of the map $D^{\frac{1}{2}}$ followed by $O$. $D^{\frac{1}{2}}$ is a pure scaling of each axis. If none of the diagonal entries are zero - i.e. if $\Sigma$ is positive-definite - this turns an origin-centered sphere (which is a contour curve of a multivariate standard normal) into an ellipsoid. This is followed by an orthogonal operation, which means that distance is preserved. The ellipsoid is now rotated and/or reflected around the origin. These are the contour curves of a regular multidimensional Gaussian. If the distribution is singular, one or more dimensions are "squashed out" - we get an elliptic "pancake" instead of an ellipsoid.

\section{Multivariate normals in block form}

\subsection{Definition}
Consider a multivariate normal on $\mathbb{R}^n$. As shown above, we can characterize such a distribution by a vector of means $\xi\in\mathbb{R}^n$ and a symmetric variance matrix $\Sigma\in\mathbb{R}^{n\times n}$. We now split $\xi$ and $\Sigma$ into \textit{block form} as follows:
\begin{equation}
\xi=
\begin{pmatrix}
\xi_1 \\ \xi_2
\end{pmatrix},\quad
\Sigma=
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\end{equation}
Here $\xi_1\in\mathbb{R}^{n_1+}, \xi_2\in\mathbb{R}^{n_2+}$ and $\Sigma_{11}\in\mathbb{R}^{n_1\times n_1}, \Sigma_{12}\in\mathbb{R}^{n_1\times n_2}, \Sigma_{21}\in\mathbb{R}^{n_2\times n_1}$, and $\Sigma_{22}\in\mathbb{R}^{n_2\times n_2}$. The transpose of $\Sigma$ is:
\begin{equation}
\Sigma^t=
\begin{pmatrix}
\Sigma_{11}^t & \Sigma_{21}^t \\
\Sigma_{12}^t & \Sigma_{22}^t
\end{pmatrix}
\end{equation}
From this, we immediately see, that since $\Sigma$ is symmetric, we have:
\begin{equation}
\Sigma_{11}=\Sigma_{11}^t, \Sigma_{12}=\Sigma_{21}^t, \Sigma_{22}=\Sigma_{2}^t
\end{equation}

\subsection{Results about multivariate normal distributions in block form}
\begin{theorem}
\label{block_split}
Let $X_1$ and $X_2$ be random vectors in $\mathbb{R}^{n_1}$ and $\mathbb{R}^{n_2}$ respectively. Now assume:
\begin{equation}
\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
\xi_1 \\ \xi_2
\end{pmatrix},
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\right)
\end{equation}
Then $X_1\sim N(\xi_1,\Sigma_{11})$ and $X_2\sim N(\xi_2,\Sigma_{22})$.
\end{theorem}
\begin{proof}
It is trivial, that both $X_1$ and $X_2$ are multivariate normal, since all linear combinations of components from each must be univariate normal because of the assumption of the block vector being multivariate normal.

Now consider the following $n_1\times n$ block matrix:
\begin{equation}
\begin{pmatrix}
I_1 & 0
\end{pmatrix}
\end{equation}
Here $I_1$ is the $n_1\times n_1$ unit matrix. Now we may express $X_1$ as follows:
\begin{equation}
X_1=\begin{pmatrix}
I_1 & 0
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\end{equation} 
The mean must then be:
\begin{equation}
\begin{pmatrix}
I_1 & 0
\end{pmatrix}
\begin{pmatrix}
\xi_1 \\ \xi_2
\end{pmatrix}
=\xi_1
\end{equation}
And the variance:
\begin{equation}
\begin{pmatrix}
I_1 & 0
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\begin{pmatrix}
I_1 \\ 0
\end{pmatrix}
=\Sigma_{11}
\end{equation}
Similarly for $X_2$.
\end{proof}

\begin{theorem}
As above, let:
\begin{equation}
\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
\xi_1 \\ \xi_2
\end{pmatrix},
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\right)
\end{equation}
Now assume $\Sigma_{22}$ to be invertible. Then the conditional distribution of $X_1$ given $X_2$ is:
\begin{equation}
X_1|_{X_2=x_2}\sim N(\xi_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\xi_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\end{equation}
\end{theorem}
\begin{proof}
Consider the following random vector:
\begin{equation}
\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
=
\begin{pmatrix}
X_1-\Sigma_{12}\Sigma_{22}^{-1}X_2 \\ X_2
\end{pmatrix}
\end{equation}
Call the upper block random vector $Z$:
\begin{equation}
Z=X_1-\Sigma_{12}\Sigma_{22}^{-1}X_2
\end{equation}
Now, we know that the random vector $\begin{pmatrix}Z \\ X_2\end{pmatrix}$ must be multivariate normal, as it's a linear transform of the multivariate normal $\begin{pmatrix}X_1 \\ X_2\end{pmatrix}$. Let us calculate the mean:
\begin{equation}
E\left[\begin{pmatrix}Z \\ X_2\end{pmatrix}\right]=
\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}\xi_1 \\ \xi_2\end{pmatrix}
=
\begin{pmatrix}
\xi_1-\Sigma_{12}\Sigma_{22}^{-1}\xi_2 \\ \xi_2
\end{pmatrix}
\end{equation}
And the variance:
\begin{align}
\textrm{Var}\left(\begin{pmatrix}Z \\ X_2\end{pmatrix}\right)=&
\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}^t=
\\
&\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}
\begin{pmatrix}
I_1 & 0 \\
-\Sigma_{22}^{-1}\Sigma_{21} & I_2
\end{pmatrix}=
\\
&\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{12} \\
\Sigma_{21}-\Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{22}
\end{pmatrix}=
\\
&\begin{pmatrix}
I_1 & -\Sigma_{12}\Sigma_{22}^{-1} \\
0   & I_2
\end{pmatrix}
\begin{pmatrix}
\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{12} \\
0 & \Sigma_{22}
\end{pmatrix}=
\\
&\begin{pmatrix}
\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{12}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22} \\
0 & \Sigma_{22}
\end{pmatrix}=
\\
&\begin{pmatrix}
\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & 0 \\
0 & \Sigma_{22}
\end{pmatrix}
\end{align}
Since the off-diagonal elements are all zero, we see that all components of $Z$ are uncorrelated with all components of $X_2$. Since the composite block vector is multinormally distributed, it follows that they are also independent. Hence, the conditional distribution of $Z$ given $X_2$ is simply the corresponding marginal distribution:
\begin{equation}
Z|_{X_2=x_2}\sim N(\xi_1-\Sigma_{12}\Sigma_{22}^{-1}\xi_2,\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\end{equation}
Here, we've utilized theorem \ref{block_split}. But remember, that $Z=X_1-\Sigma_{12}\Sigma_{22}^{-1}X_2$, or equivalently that $X_1=Z+\Sigma_{12}\Sigma_{22}^{-1}X_2$. So:
\begin{equation}
X_1|_{X_2=x_2}=Z|_{X_2=X_2}+(\Sigma_{12}\Sigma_{22}^{-1}X_2)|_{X=X_2}=Z|_{X_2=x_2}+\Sigma_{12}\Sigma_{22}^{-1}x_2
\end{equation}
This simply means a shift of the mean, so we end up with:
\begin{equation}
X_1|_{X_2=x_2}\sim N(\xi_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\xi_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\end{equation}
\end{proof}


\section{Formulation in terms of precision}

\subsection{Inner products on $\mathbb{R}^n$}
An inner product can be seen as a map $\mathbb{R}^n\times\mathbb{R}^n\rightarrow\mathbb{R}$. By the inner product axioms, this map must be linear in both variables, and so it can be expressed as a quadratic form:
\begin{equation}
\langle x,y\rangle=x^t By
\end{equation}
Here $B$ is a $n\times n$ matrix. By symmetry of the inner product, $\langle e_i,e_j\rangle=\langle e_j,e_i\rangle$ implying $B_{ij}=B_{ji}$, so $B$ is symmetric. Since $\langle x,x\rangle=x^t Bx\ge 0$, $B$ is semi-positive definite. Since the equality sign only holds when $x=0$, $B$ is also positive definite.

We will refer to such an inner product as a \textit{precision}.

\subsection{Density function}
Now, we may rewrite equation \ref{mvn_density} as follows:
\begin{equation}
f(x)=\frac{1}{\sqrt{\textrm{det}(2\pi\Sigma)}}\exp\left[-\frac{1}{2}||x-\mu||_\Sigma^2\right]
\end{equation}
Here, $||\cdot||_\Sigma$ is the norm induced by using the inner product as described above, using $\Sigma$ as $B$. So for regular multivariate normals, specifying a variance matrix or a precision are two different ways to express the same thing.

\section{Orthogonal complement}
We now wish to develop a theory of normal distributions on arbitrary inner product spaces, not just Euclidean ones. We will do this in a number of steps.

Let $V$ be a finite-dimensional vector space with an inner product $\langle\cdot,\cdot\rangle$. Let $U$ be a subspace of $V$. Then we define the \textit{orthogonal complement} of $U$ as:
\begin{equation}
U^\perp=\{v\in V|\forall u\in U: \langle u, v\rangle = 0\}
\end{equation}
\begin{theorem}
$U^\perp$ is a subspace of $V$.
\end{theorem}
\begin{proof}
According to the subspace theorem, we need to show three things:
\begin{itemize}
\item $U^\perp$ is not empty: Clearly $0\in U^\perp$.
\item Closed under addition: If $v_1,v_2\in U^\perp$, then for all $u\in U^\perp$:
\begin{equation}
\langle v_1+v_2,u\rangle=\langle v_1,u\rangle + \langle v_2,u\rangle = 0 
\end{equation}
\item Closed under scalar multiplication: If $v\in U^\perp$ and $c\in\mathbb{R}$ then for all $u\in U^\perp$:
\begin{equation}
\langle cv,u\rangle = c\langle v,u\rangle = 0
\end{equation}
\end{itemize}
\end{proof}

Since the only vector perpendicular to itself is $0$, we further conclude that $U\cap U^\perp=\{0\}$.

\begin{theorem}
If $e_1, e_2,\ldots,e_m$ is an orthonormal basis for $U$, then for any $v\in V$:
\begin{equation}
v-\sum_{i=1}^m\langle v,e_i\rangle e_i\in U^\perp
\end{equation}
\end{theorem}
\begin{proof}
Let $u\in U$. Then we can write $u=\sum_{j=1}^m\lambda_j e_j$ for some coefficients $\lambda_j$. Now calculate the inner product with the vector above:
\begin{equation}
\langle v-\sum_{i=1}^m\langle v,e_i\rangle e_i\ ,\ \sum_{j=1}^m\lambda_j e_j\rangle=\sum_{i=j}^m\lambda_j\langle v,e_j\rangle-\sum_{i=1}^m\sum_{j=1}^m\lambda_j\langle v,e_i\rangle\langle e_i,e_j\rangle
\end{equation}
Since $\langle e_i,e_j\rangle=\delta_{ij}$ this vanishes.
\end{proof}

This means that we may write any $v\in V$ as a sum of vectors from $U$ and $U^\perp$ respectively:
\begin{equation}
\label{u_uperp}
v=\underbrace{\sum_{i=1}^m\langle v,e_i\rangle e_i}_{\in U}\ +\ \underbrace{v-\sum_{i=1}^m\langle v,e_i\rangle e_i}_{\in U^\perp}
\end{equation}

\begin{theorem}
The decomposition into elements from $U$ and $U^\perp$ from equation \ref{u_uperp} is unique.
\end{theorem}
\begin{proof}
Let $v=u_1+u^\perp_1$ and $v=u_2+u^\perp_2$ be two such decompositions. Then $u_1+u^\perp_1=u_2+u^\perp_2$ and hence $u_1-u_2=u^\perp_2-u^\perp_1$. But this means that this vector is a member of both $U$ and $U^\perp$, and hence it must be $0$. This means $u_1=u_2$ and $u^\perp_1=u^\perp_2$.
\end{proof}

\subsection{The orthogonal projection}
The previous section motivates the following:
\begin{definition}
Let $V$ be a finite-dimensional inner product vector space and $U$ a subspace of $V$. The orthogonal projection from $V$ onto $U$ is the map $p:V\rightarrow V$ which satisfies:
\begin{equation}
\forall v\in V:\quad p(v)\in U,\quad v-p(v)\in U^\perp
\end{equation}
\end{definition}
As we see, one could also define the co-domain of $p$ to be $U$. Usually, the distinction will not matter much.

\begin{theorem}
The orthogonal projection operator is linear.
\end{theorem}
\begin{proof}
We need to show additivity and homogeneity:
\begin{itemize}
\item Additivity: Let $v_1,v_2\in V$. Then $p(v_1)+p(v_2)\in U$ and:
\begin{equation}
v_1-p(v_1)+v_2-p(v_2)=v_1+v_2-(p(v_1)+p(v_2))\in U^\perp
\end{equation}
Adding the two we get $v_1+v_2$. So $p(v_1+v_2)=p(v_1)+p(v_2)$.
\item Homogeneity. Let $v\in V$ and $c\in\mathbb{R}$. Then $cp(v)\in U$ and $c(v-p(v))=cv-cp(v)\in U^\perp$. Adding the two we get $cv$, so $p(cv)=cp(v)$. 
\end{itemize}
\end{proof}

\begin{theorem}
The orthogonal projection operator $p:V\rightarrow V$ is idempotent. I.e. $p\circ p=p$.
\end{theorem}
\begin{proof}
Let $v\in V$. Then $p(v)\in U$. But this means that the decomposition of $p(v)$ is $p(v)+0$. So $p\circ p(v)=p(v)$.
\end{proof}

\section{Lebesgue measures on vector spaces}

\subsection{Coordinate maps}
Let $V$ be a finite-dimensional vector space of dimension $n$. Our question is, if we can turn $V$ into a measure space in a natural way. Since we know that $V$ is isomorphic to $\mathbb{R}^n$, it makes sense to tweak the usual Lebesgue measure in $N$ dimensions:

Let $e_1,e_2,\ldots,e_n$ be a basis for $V$. Then we can define the \textit{coordinate map} as follows:
\begin{equation}
\phi:\mathbb{R}^n\rightarrow V,\quad
\begin{pmatrix}
x_1	\\	x_2	\\ \vdots	\\ x_n
\end{pmatrix}
\mapsto\sum_{i=1}^n x_i e_i
\end{equation}
This is obviously an isomorphism. Specifically, it is invertible with inverse $\phi^{-1}: V\rightarrow\mathbb{R}^n$.

The coordinate map depends on the chosen basis. If we had chosen another basis $e^*_1,e^*_2,\ldots,e^*_n$ we would get another isomorphism $\phi^*$.

\subsection{Borel algebra on $V$}
We can now use $\phi^{-1}$ to induce a $\sigma$-algebra on $V$. Set $\mathbb{B}_V$ to the smallest $\sigma$-algebra that makes $\phi^{-1}$ measurable when $\mathbb{R}^n$ is equipped with the Borel algebra $\mathbb{B}_n$. We call $\mathbb{B}_V$ the \textit{Borel algebra on $V$}.

At first this object seems to depend of the choice of basis for $V$. But it turns out that the use of definite article in the definition is justified:
\begin{theorem}
If $e_1,e_2,\ldots,e_n$ and $e^*_1,e^*_2,\ldots,e^*_n$ are bases for $V$, then the induced $\sigma$-algebra $\mathbb{B}_V$ and $\mathbb{B}^*_V$ is the same thing.
\end{theorem}
\begin{proof}
We know that $\phi^{-1}$ is $\mathbb{B}_V-\mathbb{B}_n$ measurable by definition. We have:
\begin{equation}
(\phi^*)^{-1}=(\phi^*)^{-1}\circ\textrm{id}_V=(\phi^*)^{-1}\circ(\phi\circ\phi^{-1})=\left((\phi^*)^{-1}\circ\phi\right)\circ\phi^{-1}
\end{equation}
$\left((\phi^*)^{-1}\circ\phi\right)$ is a linear operator on $\mathbb{R}^n$ and so according to section \ref{euclidean_lebesgue_properties} is measurable. So $(\phi^*)^{-1}$ must be $\mathbb{B}_V-\mathbb{B}_n$-measurable. Since $\mathbb{B}_V^*$ is the smallest $\sigma$-algebra to make $(\phi^*)^{-1}$ $\mathbb{B}_V-\mathbb{B}_n$-measurable, we must have $\mathbb{B}^*_V\subseteq\mathbb{B}_V$.

But by a totally symmetric argument, we must also have $\mathbb{B}_V\subseteq\mathbb{B}^*_V$. Hence $\mathbb{B}_V=\mathbb{B}^*_V$.
\end{proof}

It turns out, that $\phi$ must be measurable too. This is a direct consequence of the pipeline lemma.

\begin{theorem}
Given two finite-dimensional vector spaces $V$ and $W$, then:
\begin{equation}
\mathbb{B}_{V\times W}=\mathbb{B}_V\otimes\mathbb{B}_W
\end{equation}
\end{theorem}
\begin{proof}
Let $e_1, e_2,\ldots, e_n$ be a basis for $V$ with corresponding coordinate map $\phi$. And $f_1, f_2,\ldots, f_m$ a basis for $W$ with corresponding coordinate map $\psi$. Then $(e_1, 0), (e_2, 0),\ldots,(e_n,0),(0, f_1), (0, f_2),\ldots,(0,f_m)$ is a basis for $V\times W$. The corresponding coordinate map is:
\begin{equation}
\phi\times\psi: (x_1, x_2,\ldots,x_{n+m})\mapsto\left(\sum_{i=1}^n x_i e_i,\sum_{j=1}^m x_{n+j}f_j\right)
\end{equation}
The inverse is $(\phi\times\psi)^{-1}=\phi^{-1}\times\psi^{-1}$. Since $\phi^{-1}$ is $\mathbb{B}_V-\mathbb{B}_n$-measurable and $\psi^{-1}$ is $\mathbb{B}_W-\mathbb{B}_m$-measurable, $\phi^{-1}\times\psi^{-1}$ must be $\mathbb{B}_V\otimes\mathbb{B}_W-\mathbb{B}_n\otimes\mathbb{B}_n$-measurable. But $\mathbb{B}_n\otimes\mathbb{B}_m=\mathbb{B}_{n+m}$. Since $\mathbb{B}_{V\times W}$ is the smallest $\sigma$-algebra to make $\phi^{-1}\times\psi^{-1}$ measurable, we must have $\mathbb{B}_{V\times W}\subseteq\mathbb{B}_V\otimes\mathbb{B}_W$.

On the other hand, consider the projection operators:
\begin{align}
\pi_V:& V\times W\rightarrow V, (v,w)\mapsto v\\
\pi_n:& \mathbb{R}^{n+m}\rightarrow\mathbb{R}^n, (x_1,\ldots,x_{n+m})\mapsto(x_1,\ldots,x_n)
\end{align}
Now consider $\pi_V\circ(\phi\times\psi)$. Applied to an $x\in\mathbb{R}^{n+m}$ we have:
\begin{equation}
\pi_V\circ(\phi\times\psi)(x)=\pi_v\left(\left(\sum_{i=1}^n x_i e_i,\sum_{j=1}^m x_{n+j}f_j\right)\right)=\sum_{i=1}^n x_i e_i
\end{equation}
But this is the same as:
\begin{equation}
\phi\circ\pi_1(x)=\phi\left((x_1,\ldots x_n)\right)=\sum_{i=1}^n x_i e_i
\end{equation}
So $\pi_V\circ(\phi\times\psi)=\phi\circ\pi_1$. Now apply $\phi^{-1}\times\psi^{-1}$ from the right to get:
\begin{equation}
\pi_V=\phi\circ\pi_1\circ(\phi^{-1}\times\psi^{-1})
\end{equation}
Since all the three functions on the right side are measurable, $\pi_V$ must be $\mathbb{B}_{V\times W}-\mathbb{B}_V$-measurable. By a similar argument the corresponding projection operator $\pi_W: V\times W\rightarrow W$ is $\mathbb{B}_{V\times W}-\mathbb{B}_W$-measurable. Since $\mathbb{B}_V\otimes\mathbb{B}_W$ is the smallest $\sigma$-algebra to make both $\pi_V$ and $\pi_W$ measurable, we must have: $\mathbb{B}_V\otimes\mathbb{B}_W\subseteq\mathbb{B}_{V\times W}$.
\end{proof}

\subsection{Lebesgue measures on $V$}
We now want to define a measure on the measurable space $(V,\mathbb{B}_V)$. If $e_1, e_2,\ldots, e_n$ is a basis for $V$, we will use the associated coordinate map $\phi$ to define a measure:
\begin{equation}
\lambda_V=\phi(m_n)
\end{equation}
Here, $m_n$ is the usual Lebesgue measure in $n$ dimensions. The problem is, that this measure depends on the chosen basis! Consider another basis $e^*_1, e^*_2,\ldots,e^*_n$ and associated coordinate map $\phi^*$. Then the measure is:
\begin{equation}
\lambda^*_V=\phi^*(m_n)=(\phi\circ\phi^{-1})\circ\phi^*(m_n)=\phi\circ(\phi^{-1}\circ\phi^*(m_n))
\end{equation}
Now $\phi^{-1}\circ\phi^*$ is an isomorphism $\mathbb{R}^n\rightarrow\mathbb{R}^n$, so according to section \ref{euclidean_lebesgue_properties}, there is a constant $c$ such that $(\phi^{-1}\circ\phi^*(m_n))=c m_n$. So:
\begin{equation}
\lambda^*_V=c\phi(m_n)=c\lambda_V
\end{equation}

So while there are many Lebesgue measures on $V$ they only differ from each other by a constant factor. This means that they all agree on what constitutes a null set, and on which functions are integrable. They disagree on the integral, but agree on whether it is finite or not. The also agree on whether a measure $\mu$ has a density with respect to $\lambda_V$ or not.

\section{Regular normal distributions on $V$}

\subsection{Gaussian integrals on $V$}
Let $V$ be an inner product space of finite dimension $n$. Let $a_1, a_2,\ldots, a_n$ be an orthogonal basis for $V$, $\phi:\mathbb{R}^n\rightarrow V$ be the corresponding coordinate transformation, and $\lambda$ be the corresponding Lebesgue measure on $V$. We're now interested in evaluating the following integral:
\begin{equation}
\int_V\exp\left[-\frac{1}{2}||v||^2\right]d\lambda(v)
\end{equation}
Let's start by writing a general vector $v$ as $v=\sum_{i=1}^n x_i a_i$. Then:
\begin{equation}
||v||^2=\langle v,v\rangle=\langle\sum_{i=1}^n x_i a_i,\sum_{j=1}^n x_j a_j\rangle=\sum_{i=1}^n\sum_{j=1}^n x_i x_j\langle a_i,a_j\rangle
\end{equation}
Since all the $a_i$'s are mutually orthogonal this means:
\begin{equation}
||v||^2=\sum_{i=1}^n x_i^2||a_i||^2
\end{equation}
So we may write:
\begin{equation}
\int_V\exp\left[-\frac{1}{2}||v||^2\right]d\lambda(v)=\int_{\mathbb{R}^n}\exp\left[-\frac{1}{2}\sum_{i=1}^n x_i^2||a_i||^2\right]dm_n(x)
\end{equation}
Using Tonelli's theorem this is equal to:
\begin{equation}
\prod_{i=1}^n\int_\mathbb{R}\exp\left[-\frac{1}{2}x_i^2||a_i||^2\right]dm_1(x_i)=\prod_{i=1}^n\frac{\sqrt{2\pi}}{||a_i||}=\frac{(2\pi)^{n/2}}{\prod_{i=1}^n||a_i||}
\end{equation}

\end{document}