\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png, .jpg}

\title{General feed forward neural networks}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Formalism and nomenclature}
Consider a feed forward neural net with $L+1$ layers, in the sense that layer zero is the input layer and layer $L$ the output layer. Each non-input layer has its own activation function $\sigma_l$.

The size of layer $l$ we will denote $S_l$. So layer $l$ has a $S_l\times S_{l-1}$ matrix of weights $W^{(l)}$ and a bias vector $b^{(l)}$ with dimension $S_l$. So, given an input vector $x$ (with dimension $S_{l-1}$), the pre-activation and the activation of layer $l$ can be expressed as:
\begin{equation}
\label{forward_propagation}
z^{(l)}=W^{(l)} x + b^{(l)},\quad a^{(l)}=\sigma_l(z^{(l)})=\sigma_l(W^{(l)} x + b^{(l)})
\end{equation}

We will consider $N$ data points, each with a feature vector of dimension $S_0$. We group these into an $S_0\times N$ matrix $X$:
\begin{equation}
X=
\begin{pmatrix}
| & \cdots & | \\
x_1 & \cdots & x_N \\
| & \cdots & |
\end{pmatrix}
\end{equation}
Note that this is the transpose of the usual "data frame" structure.

We can now write the pre-activations and activations of the first layer as:
\begin{equation}
Z^{(1)}=W^{(1)} X+b^{(1)},\quad A^{(1)}=\sigma_1(Z^{(1)})=\sigma_1(W^{(1)} X+b^{(1)})
\end{equation}
Here, we've used "$+b^{(1)}$" as a shorthand for adding the vector $b^{(1)}$ to every column. We could write this as "$+b^{(1)} J^t_{N}$" if we wanted to be accurate. ($J_N$ is a column vector of $N$ ones).

Similarly, we may generally write the pre-activations and activations of layer $l$ as:
\begin{equation}
Z^{(l)}=W^{(l)} A^{(l-1)}+b^{(l)},\quad A^{(l)}=\sigma_l(Z^{(l)})=\sigma_l(W^{(l)} A^{(l-1)}+b^{(l)})
\end{equation}
We will also identify $X$ with the activations of "layer zero": $A^{(0)}=X$.

Finally, we have a cost function $J$ which measures the distance to some target data $T\in\mathbb{R}^{S_L\times N}$:
\begin{equation}
T=
\begin{pmatrix}
| & \cdots & | \\
t_1 & \cdots & t_N \\
| & \cdots & |
\end{pmatrix}
\end{equation}
We will assume the cost function is of the form $J=J(T,A^{(L)})$, taking on real values. I.e. it only depends on the targets and the activations of the output layer. We will make further assumptions about $J$ later.

\section{Backpropagation - Output layer}
Forward propagation through the network is described by equation \ref{forward_propagation}. The procedure is assumed to be done before we look at how to determine partial derivatives of $J$ through backpropagation.

\subsection{Weights}
The derivatives with respect to the output layer weights and biases can be found through the chain rule:
\begin{equation}
\label{output_grad}
\frac{\partial J}{\partial W^{(L)}_{ij}}=\sum_{k=1}^{S_L}\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{kn}}\frac{\partial A^{(L)}_{kn}}{\partial W^{(L)}_{ij}},\quad\frac{\partial J}{\partial b^{(L)}_i}=\sum_{k=1}^{S_L}\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{kn}}\frac{\partial A^{(L)}_{kn}}{\partial b^{(L)}_i}
\end{equation}
We may find the derivatives of $A^{(L)}$ with respect to the weights and biases:
\begin{equation}
\frac{\partial A^{(L)}_{kn}}{\partial W^{(L)}_{ij}}=\frac{\partial A^{(L)}_{kn}}{\partial\sigma_L}\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}\frac{\partial Z^{(L)}_{kn}}{\partial W^{(L)}_{ij}}
\end{equation}
But since $\frac{\partial A^{(L)}_{kn}}{\partial\sigma_L}$ is simply one, this reduces to:
\begin{equation}
\label{output_a_grad}
\frac{\partial A^{(L)}_{kn}}{\partial W^{(L)}_{ij}}=\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}\frac{\partial Z^{(L)}_{kn}}{\partial W^{(L)}_{ij}}
\end{equation}
This also means that when writing the $J$-derivative in matrix form, there will be a Hadamard product between the first two terms instead of ordinary matrix multiplication:
\begin{equation}
\label{hadamard_diff}
\frac{\partial J}{\partial W^{(L)}}=\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\frac{\partial Z^{(L)}}{\partial W^{(L)}}
\end{equation}

Finally, we may calculate the derivatives of $Z^{(L)}$ with respect to the weights:
\begin{equation}
\frac{\partial Z^{(L)}_{kn}}{\partial W^{(L)}_{ij}}=\frac{\partial}{\partial W^{(L)}_{ij}}(W^{(L)}A^{(L-1)}+b^{(L)})_{kn}=\frac{\partial}{\partial W^{(L)}_{ij}}\left(\sum_{l=1}^{S_{L-1}}W^{(L)}_{kl}A^{(L-1)}_{ln}+b^{(L)}_k\right)
\end{equation}
Differentiating $W^{(L)}$ with respect to $W^{(L)}$ yields two Kronecker deltas:
\begin{equation}
\label{output_z_grad}
\frac{\partial Z^{(L)}_{kn}}{\partial W^{(L)}_{ij}}=\sum_{l=1}^{S_{L-1}}\delta_{ik}\delta_{jl}A^{(L-1)}_{ln}=\delta_{ik} A^{(L-1)}_{jn}
\end{equation}
Now, we may insert equations \ref{output_a_grad} and \ref{output_z_grad} into \ref{output_grad}:
\begin{equation}
\frac{\partial J}{\partial W^{(L)}_{ij}}=\sum_{k=1}^{S_L}\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{kn}}\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}\delta_{ik} A^{(L-1)}_{jn}=\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{in}}\frac{\partial\sigma_L}{\partial Z^{(L)}_{in}}A^{(L-1)}_{jn}
\end{equation}
We can rewrite this using the Hadamard product between the two derivatives and swapping the indices of $A^{(L-1)}$, turning into a transpose:
\begin{equation}
\sum_{n=1}^N\left[\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\right]_{in}\left(A^{(L-1)}\right)^t_{nj}
\end{equation}
Or in matrix form:
\begin{equation}
\label{output_weights_matrix}
\frac{\partial J}{\partial W^{(L)}}=\left[\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\right]\left(A^{(L-1)}\right)^t=H^{(L)}\left(A^{(L-1)}\right)^t
\end{equation}
Here we've introduced $H^{(L)}$, the matrix of the Hadamard product, for notational ease:
\begin{equation}
H^{(L)}=\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}
\end{equation}

\subsection{Biases}
The procedure for the biases is the same until we get to the $Z^{(L)}$ derivative:
\begin{equation}
\frac{\partial Z^{(L)}_{kn}}{\partial b^{(L)}_i}=\frac{\partial}{\partial b^{(L)}_i}\left(\sum_{l=1}^{S_{L-1}}W^{(L)}_{kl}A^{(L-1)}_{ln}+b^{(L)}_k\right)=\delta_{ik}
\end{equation}
Reinserting all the way back to equation \ref{output_grad} we get:
\begin{equation}
\frac{\partial J}{\partial b^{(L)}_i}=\sum_{k=1}^{S_L}\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{kn}}\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}\delta_{ik}=\sum_{n=1}^N\frac{\partial J}{\partial A^{(L)}_{in}}\frac{\partial\sigma_L}{\partial Z^{(L)}_{in}}
\end{equation}
Again, we can write this is matrix notation using the Hadamard product:
\begin{equation}
\label{output_bias_matrix}
\frac{\partial J}{\partial b^{(L)}}=\left[\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\right]J_N^t=H^{(L)}J_N^t
\end{equation}

\subsection{Output layer "error"}
We will call the quantity $\frac{\partial J}{\partial A^{(L)}}$ the output layer "error":
\begin{equation}
\Delta^{(L)}=\frac{\partial J}{\partial A^{(L)}}
\end{equation}
The quotes are used, because there's no need for this to be equal/proportional to what we usually call errors, i.e. distance between the output $A^{(L)}$ and $T$. However, often this is the case (or rather, $J$ is specifically chosen to make $\Delta$, $H$, or $\delta$ defined below equal/proportional to it - see below). At any rate, we may now write:
\begin{align}
\label{output_weights_H}
H^{(L)}=&\Delta^{(L)}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\\
\frac{\partial J}{\partial W^{(L)}}=&H^{(L)}\left(A^{(L-1)}\right)^t=\left[\Delta^{(L)}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\right]\left(A^{(L-1)}\right)^t\\
\label{output_bias_H}
\frac{\partial J}{\partial b^{(L)}}=&H^{(L)}J_N^t=\left[\Delta^{(L)}\odot\frac{\partial\sigma_L}{\partial Z^{(L)}}\right]J_N^t
\end{align}

\section{The cost function}
Let's take a closer look at the cost function. Usually, it can be written as an average of a \textit{loss function} $\mathcal{L}(t,a)$, where $t$ and $a$ are the desired and actual activations for the output layer. Then the cost function can be written:
\begin{equation}
J(T,A^{(L)})=\frac{1}{N}\sum_{i=1}^N\mathcal{L}(t_i,a^{(L)}_i)
\end{equation}
The "error" term is now:
\begin{equation}
\Delta^{(L)}=\frac{\partial J}{\partial A^{(L)}}=\frac{1}{m}\sum_{i=1}^N\frac{\partial}{\partial A^{(L)}}\mathcal{L}(t_i,a^{(L)}_i)
\end{equation}
The derivative is only non-zero when differentiating with respect to $a^{(L)}_i$, so this is the same as:
\begin{equation}
\Delta^{(L)}=\frac{1}{m}\sum_{i=1}^N\underbrace{\frac{\partial}{\partial a^{(L)}_i}\mathcal{L}(t_i,a^{(L)}_i)}_{\delta^{(L)}_i}
\end{equation}
Here, we've defined the error function for specific a data point $\delta^{(L)}_i$.

\subsection{Example: Euclidean distance cost function}
A common form of cost function is:
\begin{equation}
J(T,A^{(L)})=\frac{1}{2N}\sum_{m=1}^N||a^{(L)}_m-t_m||^2
\end{equation}
Here, $a^{(L)}_m$ is the $m$'th column of $A^{(L)}$ and the double dashes is the usual Euclidean norm in $S_L$ dimensions. So with the notation above:
\begin{equation}
\mathcal{L}(t,a^{(L)})=\frac{1}{2}||a^{(L)}-t||^2
\end{equation}
The error for a data point is:
\begin{equation}
\delta^{(L)}_i=\frac{1}{2}\frac{\partial}{\partial a^{(L)}_i}||a^{(L)}_i-t_i||^2=a^{(L)}_i-t_i
\end{equation}
This conforms with the usual notion of error for a data point. The total "error" term becomes:
\begin{equation}
\Delta^{(L)}=\frac{1}{N}\left(A^{(L)}-T\right)
\end{equation}

\subsection{Example: Cross-entropy cost function with logistic sigmoid activation function}
\label{coursera_case}
Here the cost function is:
\begin{equation}
J(T,A^{(L)})=-\frac{1}{N}\sum_{m=1}^N\left[t_i\log a^{(L)}_i+(1-t_i)\log(1-a^{(L)}_i)\right]
\end{equation}
Or in other words:
\begin{equation}
\mathcal{L}(t,a^{(L)})=-\left[t\log a^{(L)}+(1-t)\log(1-a^{(L)})\right]
\end{equation}
The data point error is:
\begin{equation}
\delta^{(L)}_i=-\left[\frac{t_i}{a^{(L)}_i}-\frac{1-t_i}{1-a^{(L)}_i}\right]=\frac{-t_i(1-a^{(L)}_i)+(1-t_i)a^{(L)}_i}{a^{(L)}_i(1-a^{(L)}_i)}=\frac{a^{(L)}_i-t_i}{a^{(L)}_i(1-a^{(L)}_i)}
\end{equation}
This looks like what we usually think of as the error, except for the denominator. However, we also see that the same denominator is equal to the derivative of a logistic sigmoid function. Hence we will re-find the familiar from when we get to the Hadamard product. Let's start with $\Delta$:
\begin{equation}
\Delta^{(L)}=\frac{1}{N}\sum_{i=1}^N\frac{a^{(L)}_i-t_i}{a^{(L)}_i(1-a^{(L)}_i)}
\end{equation}
As noted above, for $H$ the denominator cancels and we get:
\begin{equation}
H^{(L)}=\frac{1}{N}(A^{(L)}-T)
\end{equation}

\section{Backpropagation - Last hidden layer}

\subsection{Weights}
Now, let's consider derivatives with respect to weights in layer $L-1$. Simply applying the chain rule, we get:
\begin{equation}
\label{last_hidden_weights}
\frac{\partial J}{\partial W^{(L-1)}_{ij}}=\sum_{k=1}^{S_L}\sum_{n=1}^N\sum_{k'=1}^{S_{L-1}}\sum_{n'=1}^N\frac{\partial J}{\partial A^{(L)}_{kn}}\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}\frac{\partial Z^{(L)}_{kn}}{\partial A^{(L-1)}_{k'n'}}\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}_{k'n'}}\frac{\partial Z^{(L-1)}_{k'n'}}{\partial W^{(L-1)}_{ij}}
\end{equation}
Now, this will obviously be very similar to the calculations above, but to be certain, let's proceed carefully. The only term in the above we have not calculated yet is:
\begin{align}
\frac{\partial Z^{(L)}_{kn}}{\partial A^{(L-1)}_{k'n'}}=&\frac{\partial}{\partial A^{(L-1)}_{k'n'}}\left(W^{(L)}A^{(L-1)}+b^{(L)}\right)_{kn}=\\
&\frac{\partial}{\partial A^{(L-1)}_{k'n'}}\left(\sum_{l=1}^{S_{L-1}}W^{(L)}_{kl}A^{(L-1)}_{ln}+b^{(L)}_k\right)=\\
&\sum_{l=1}^{S_{L-1}}W^{(L)}_{kl}\delta_{lk'}\delta_{nn'}=W^{(L)}_{kk'}\delta_{nn'}
\end{align}
Now we're ready to insert into equation \ref{last_hidden_weights}:
\begin{align}
\frac{\partial J}{\partial W^{(L-1)}_{ij}}=&\sum_{k=1}^{S_L}\sum_{n=1}^N\sum_{k'=1}^{S_{L-1}}\sum_{n'=1}^N\Delta^{(L)}_{kn}\frac{\partial\sigma_L}{\partial Z^{(L)}_{kn}}W^{(L)}_{kk'}\delta_{nn'}\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}_{k'n'}}\delta_{ik'}A^{(L-2)}_{jn'}=\\
&\sum_{k=1}^{S_L}\sum_{n=1}^N H^{(L)}_{kn}W^{(L)}_{ki}\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}_{in}}A^{(L-2)}_{jn}
\end{align}
Use the trick of rearranging terms and swapping indices:
\begin{align}
\frac{\partial J}{\partial W^{(L-1)}_{ij}}=&\sum_{k=1}^{S_L}\sum_{n=1}^N\left(W^{(L)}\right)^t_{ik}H^{(L)}_{kn}\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}_{in}}\left(A^{(L-2)}\right)^t_{nj}=\\
&\sum_{n=1}^N\left[\left(W^{(L)}\right)^t H^{(L)}\right]_{in}\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}_{in}}\left(A^{(L-2)}\right)^t_{nj}
\end{align}
Once again, we can collect the first two terms into a Hadamard product:
\begin{align}
\frac{\partial J}{\partial W^{(L-1)}_{ij}}=&\sum_{n=1}^N\left[\left(W^{(L)}\right)^t H^{(L)}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}\right]_{in}\left(A^{(L-2)}\right)^t_{nj}=\\
&\left[\left(W^{(L)}\right)^t H^{(L)}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}\left(A^{(L-2)}\right)^t\right]_{ij}
\end{align}
Or in matrix form:
\begin{equation}
\label{last_hidden_weights_matrix}
\frac{\partial J}{\partial W^{(L-1)}}=\left[\underbrace{\left(W^{(L)}\right)^t H^{(L)}}_{\Delta^{(L-1)}}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}\right]\left(A^{(L-2)}\right)^t
\end{equation}
Here, we've defined the underbraced part to be the "error" for layer $L-1$. The formula now takes a form very similar to equation \ref{output_weights_H}:
\begin{equation}
\frac{\partial J}{\partial W^{(L-1)}}=\left[\Delta^{(L-1)}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}\right]\left(A^{(L-2)}\right)^t=H^{(L-1)}\left(A^{(L-2)}\right)^t
\end{equation}
Here, we've defined the $H$ for layer $L-1$ as:
\begin{equation}
H^{(L-1)}=\Delta^{(L-1)}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}
\end{equation}

\subsection{Biases}
This will be very similar to the weights case. The only thing that changes, is that the last term in the chain rule decomposition is:
\begin{equation}
\frac{\partial Z^{(L-1)}_{k'n'}}{\partial b^{(L-1)}_i}=\delta_{ik'}
\end{equation}
So all of the calculations play out the same way as above, except there's no multiplication by $A^{(L-2)}$. Instead we get:
\begin{equation}
\frac{\partial J}{\partial b^{(L-1)}_i}=\sum_{n=1}^N\left[\left(W^{(L)}\right)^t H^{(L)}\odot\frac{\partial\sigma_{L-1}}{\partial Z^{(L-1)}}\right]_{in}=\sum_{n=1}^N H^{(L-1)}_{in}
\end{equation}
Or in matrix form:
\begin{equation}
\frac{\partial J}{\partial b^{(L-1)}}=H^{(L-1)}J^t_N
\end{equation}

\section{Backpropagation - General layer}
Here, we wish to prove that in general, the formula for derivatives of $J$ with respect to weights and biases from any layer $l$ can be written:
\begin{equation}
\frac{\partial J}{\partial W^{(l)}}=H^{(l)}\left(A^{(l-1)}\right)^t,\quad \frac{\partial J}{\partial b^{(l)}}=H^{(l)}J_N^t
\end{equation}
Here, $H^{(l)}$ is defined recursively:
\begin{equation}
H^{(l)}=\Delta^{(l)}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}},\quad \Delta^{(l)}=\left(W^{(l+1)}\right)^t H^{(l+1)},\quad \Delta^{(L)}=\frac{\partial J}{\partial A^{(L)}}
\end{equation}

\subsection{A useful lemma}
To show this it is turns out to be useful to start by proving the following:
\begin{equation}
\frac{\partial J}{\partial A^{(l)}}=\Delta^{(l)}
\end{equation}
This is done through induction, although backwards from $l=L$ down to $l=1$.

\subsubsection{Induction start}
This is corresponds to $l=L$. Here, this is true by definition:
\begin{equation}
\frac{\partial J}{\partial A^{(L)}}=\Delta^{(L)}
\end{equation}

\subsubsection{Induction step}
So we need to prove $(l)\Rightarrow(l-1)$. Notice the following:
\begin{align}
\frac{\partial J}{\partial A^{(l)}}=&\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L})}\frac{\partial Z^{(L)}}{\partial A^{(L-1)}}\odot\cdots\odot\frac{\partial\sigma_{l+1}}{\partial Z^{(l+1})}\frac{\partial Z^{(l+1)}}{\partial A^{(l)}}\\
\frac{\partial J}{\partial A^{(l-1)}}=&\underbrace{\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L})}\frac{\partial Z^{(L)}}{\partial A^{(L-1)}}\odot\cdots\odot\frac{\partial\sigma_{l+1}}{\partial Z^{(l+1})}\frac{\partial Z^{(l+1)}}{\partial A^{(l)}}}_{\frac{\partial J}{\partial A^{(l)}}}\odot\frac{\partial\sigma_{l}}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial A^{(l-1)}}
\end{align}
The Hadamard products follow from the same logic that led to equation \ref{hadamard_diff}. By the induction assumption, $\frac{\partial J}{\partial A^{(l)}}=\Delta^{(l)}$, so we need to calculate:
\begin{equation}
\frac{\partial J}{\partial A^{(l-1)}}=\Delta^{(l)}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial A^{(l-1)}}
\end{equation}
In coordinate form:
\begin{equation}
\left[\frac{\partial J}{\partial A^{(l-1)}}\right]_{in}=\sum_{j=1}^{S_l}\sum_{n'=1}^N\left(\Delta^{(l)}_{jn'}\frac{\partial\sigma_l}{\partial Z^{(l)}_{jn'}}\right)\frac{\partial Z^{(l)}_{jn'}}{\partial A^{(l-1)}_{in}}
\end{equation}
But we've already calculated the last derivative:
\begin{equation}
\sum_{j=1}^{S_l}\sum_{n'=1}^N\left(\Delta^{(l)}_{jn'}\frac{\partial\sigma_l}{\partial Z^{(l)}_{jn'}}\right)W^{(l)}_{ji}\delta_{nn'}=\sum_{j=1}^{S_l}\left(\Delta^{(l)}_{jn}\frac{\partial\sigma_l}{\partial Z^{(l)}_{jn}}\right)W^{(l)}_{ji}
\end{equation}
Now use the usual trick of swapping indices:
\begin{equation}
\left[\frac{\partial J}{\partial A^{(l-1)}}\right]_{in}=\sum_{j=1}^{S_l}\left(W^{(l)}\right)^t_{ij}\left(\Delta^{(l)}_{jn}\frac{\partial\sigma_l}{\partial Z^{(l)}_{jn}}\right)=\left[\left(W^{(l)}\right)^t H^{(l)}\right]_{ij}
\end{equation}
This is exactly the $ij$'th element of $\Delta^{(l-1)}$, as desired.

\subsection{Weights}
The lemma makes it easy to derive the formula for weights in the $l$'th layer:
\begin{align}
\frac{\partial J}{\partial W^{(l)}}=&\underbrace{\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L})}\frac{\partial Z^{(L)}}{\partial A^{(L-1)}}\odot\cdots\odot\frac{\partial\sigma_{l+1}}{\partial Z^{(l+1})}\frac{\partial Z^{(l+1)}}{\partial A^{(l)}}}_{\frac{\partial J}{\partial A^{(l)}}}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial W^{(l)}}=\\
&\Delta^{(l)}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial W^{(l)}}
\end{align}
Element-wise, using all the (by now) usual tricks:
\begin{align}
\frac{\partial J}{\partial W^{(l)}_{ij}}=&\sum_{k=1}^{S_l}\sum_{n=1}^N\Delta^{(l)}_{kn}\frac{\partial\sigma_l}{\partial Z^{(l)}_{kn}}\frac{\partial Z^{(l)}_{kn}}{\partial W^{(l)}_{ij}}=\sum_{k=1}^{S_l}\sum_{n=1}^N\Delta^{(l)}_{kn}\frac{\partial\sigma_l}{\partial Z^{(l)}_{kn}}A^{(l)}_{jn}\delta_{ki}=\\
&\sum_{n=1}^N\Delta^{(l)}_{in}\frac{\partial\sigma_l}{\partial Z^{(l)}_{in}}A^{(l)}_{jn}=\sum_{n=1}^N\Delta^{(l)}_{in}\frac{\partial\sigma_l}{\partial Z^{(l)}_{in}}\left(A^{(l)}\right)^t_{nj}=\\
&\sum_{n=1}^N\left[\Delta^{(l)}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\right]_{in}\left(A^{(l)}\right)^t_{nj}
\end{align}
Back in matrix form:
\begin{equation}
\frac{\partial J}{\partial W^{(l)}}=H^{(l)}\left(A^{(l)}\right)^t
\end{equation}

\subsection{Biases}
This is almost the same as for the weights:
\begin{align}
\frac{\partial J}{\partial b^{(l)}}=&\underbrace{\frac{\partial J}{\partial A^{(L)}}\odot\frac{\partial\sigma_L}{\partial Z^{(L})}\frac{\partial Z^{(L)}}{\partial A^{(L-1)}}\odot\cdots\odot\frac{\partial\sigma_{l+1}}{\partial Z^{(l+1})}\frac{\partial Z^{(l+1)}}{\partial A^{(l)}}}_{\frac{\partial J}{\partial A^{(l)}}}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial b^{(l)}}=\\
&\Delta^{(l)}\odot\frac{\partial\sigma_l}{\partial Z^{(l)}}\frac{\partial Z^{(l)}}{\partial b^{(l)}}
\end{align}
Elementwise:
\begin{equation}
\left[\frac{\partial J}{\partial b^{(l)}}\right]_i=\sum_{j=1}^{S_l}\sum_{n=1}^N\Delta^{(l)}_{jn}\frac{\partial\sigma_l}{\partial Z^{(l)}_{jn}}\delta_{ji}=\sum_{n=1}^N\Delta^{(l)}_{in}\frac{\partial\sigma_l}{\partial Z^{(l)}_{in}}=\sum_{n=1}^N H^{(l)}_{in}
\end{equation}
In matrix form:
\begin{equation}
\frac{\partial J}{\partial b^{(l)}}=H^{(l)}J^t_N
\end{equation}

\section{Example: Two-layer tanh-network for binary classification}
This example is from week 3 of the Coursera course "Neural Networks and Deep Learning" by Andrew Ng. The input layer has 2 dimensions, and the output 1. Since it is a binary classification model, the output activation function is sigmoid, while the hidden layer is tanh:
\begin{equation}
\sigma_1(z)=\tanh(z),\quad\sigma_2(z)=\sigma(z)
\end{equation}
The training set consists of 400 data points, so $X=\in\mathbb{R}^{2\times 400}$. The labels $T$ are called $Y$, and are one-hot encodings of the true condition, so $T=Y\in\mathbb{R}^{1\times 400}$.
The hidden layer has $n_h$ layers, so the dimensions of the weights and biases are:
\begin{equation}
W^{(1)}\in\mathbb{R}^{n_h\times 2},\quad b^{(1)}\in\mathbb{R}^{n_h\times 1},\quad
W^{(2)}\in\mathbb{R}^{1\times n_h},\quad b^{(2)}\in\mathbb{R}^{1\times 1}
\end{equation}
The loss function is the average cross-entropy for the training set. This means that we're in the same case as in section \ref{coursera_case}. Therefore the Hadamard term for the output layer is:
\begin{equation}
H^{(2)}=\frac{1}{m}(A^{(2)}-Y)
\end{equation}
So, the $W^{(2)}$-derivative is:
\begin{equation}
\frac{\partial J}{\partial W^{(2)}}=H^{(2)}A^{(1)}=\frac{1}{m}\sum_{i=1}^m\left(A^{(2)}_i-Y_i\right)(A^{(1)}_i)^t
\end{equation}
And the $b^{(2)}$-derivative:
\begin{equation}
\frac{\partial J}{\partial b^{(2)}}=H^{(2)}A^{(1)}=\frac{1}{m}\sum_{i=1}^m\left(A^{(2)}_i-Y_i\right)
\end{equation}
Now, we backpropagate the error term:
\begin{equation}
\Delta^{(1)}=(W^{(2)})^t H^{(2)}=\frac{1}{m}(W^{(2)})^t(A^{(2)}-Y)
\end{equation}
And using the derivative of $\tanh$, the Hadamard term is:
\begin{equation}
H^{(1)}_i=\Delta^{(1)}_i(1-(a^{(1)}_i)^2)
\end{equation}
Now the derivatives of $W^{(1)}$ and $b^{(1)}$ follow the usual formulas:
\begin{equation}
\frac{\partial J}{\partial W^{(1)}}=H^{(1)}(A^{(1)})^t,\quad
\frac{\partial J}{\partial b^{(1)}}=H^{(1)}(J_2)^t
\end{equation}

\end{document}