\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png}

\title{Recurrent Neural Networks}
\author{Kristian Wichmann}

\begin{document}
\maketitle

This follows Goodfellow and Bengio's book, borrowing/adapting a lot of the illustrations from this.

\section{Simple cyclical graph}

Image a situation, where a state $s=s^{(t)}$ depends on "time" $t$. Here $t$ is discrete, a whole number, usually, running from 1 to $\tau$. Further assume, that each time step is connected to the next through the same function:
\begin{equation}
s^{(t)}=f(s^{(t-1)},\theta)
\end{equation}
I.e., there is only an explicit dependence on the state from the last time step. Here $\theta$ is a shorthand for any parameters in the model. The unfolded graph of this model is shown in figure \ref{fig:cyclical_graph}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{cyclical_graph}
\caption{Simple cyclical graph.}
\label{fig:cyclical_graph}
\end{figure}

When there is a first time step, $t=1$, we see that every other state is determined by $s^{(1)}$:
\begin{equation}
s^{(t)}=f(s^{(t-1)})=f(f(s^{(t-2)}))=\cdots=f(f(\cdots f(s^{(1)})\cdots))\equiv g(s^{(1)})
\label{explicit_s1_dependence}
\end{equation}

\subsection{Cyclical graph with input}
To elaborate on this model, assume that at each time step, there is also some kind of input $x^{(t)}$. So now the state - now called $h^{(t)}$ for hidden state - depends on this as well:

\begin{equation}
s^{(t)}=f(s^{(t-1)}, x^{(t)},\theta)
\end{equation}

Figure \ref{fig:cyclical_with_input} shows both the folded and unfolded versions of this model.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{cyclical_with_input}
\caption{Cyclical graph with input.}
\label{fig:cyclical_with_input}
\end{figure}

When there is a first time step, $t=1$, we can express a general hidden state similarly to equation \ref{explicit_s1_dependence}:
\begin{align}
h^{(t)}=&f(h^{(t-1)}, x^{(t)})=f(f(h^{(t-2)}, x^{(t-1)}), x^{(t)})=\cdots\\
&f(f(\cdots f(h^{(1)},x^{(1)})\cdots,x^{(t)})\cdots))\equiv\\
&g(h^{(1)},x^{(1)},\cdots,x^{(t)})
\end{align}
So here, any the hidden state at a time $t$ depends on the hidden state at $t=1$ and all the input up to the point $t$.

\subsection{Cyclical graph with both input and output}
Finally, we would also like to model to provide some kind of output. We will assume there's output at every step, but in some applications, such as classification, we usually only care about the final output. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{cyclical_input_output}
\caption{Cyclical graph with input as well as output.}
\label{fig:cyclical_input_output}
\end{figure}

Such a model is shown in figure \ref{fig:cyclical_input_output}. Here, the output at time $t$ depends only on the hidden state:
\begin{equation}
o^{(t)}=o(h^{(t)})
\end{equation}

\section{Recurrent neural networks}
As in feed forward neural networks, we will consider cases where the functions involved are linear followed by some kind of non-linearity.

\subsection{Loss function}
As usual with neural networks, we need some kind of loss function to minimize. In general, this will be a function of the outputs:
\begin{equation}
L=L(o^{(1)},\ldots,o^{(\tau)},\mathbf{y})
\end{equation}
Here $\mathbf{y}$ is a label or collection of labels. Sometimes, it will be a function of all the outputs, sometimes only the last. In any case, we will need to know the derivatives of the outputs with respect to the parameters in the model.

\subsection{Tanh activation}
Now, consider a case where the activation function for the hidden layer is tanh. This is customary for RNNs. So, the model is:
\begin{align}
a^{(t)}&=W h^{(t-1)}+U x^{(t)}+b\\
h^{(t)}&=\tanh(a^{(t)})\\
o^{(t)}&=V h^{(t)}+c
\end{align}
In general, $a$, $h$, and $o$'s are vectors, and $U$, $V$ and $W$ are matrices of appropriate sizes.

\subsection{One-dimensional input, hidden, and output}
Consider now, as a warm-up, the case where everything is numbers instead of vectors/matrices.

For this model, we will apply softmax to the output and use a cross-entropy loss function between the result and a sequence of labels:
\begin{align}
\hat{y}^{(t)}&=\textrm{softmax}(o^{(t)})\\
L^{(t)}&=\sum_{i=1}^\tau
\end{align}



\end{document}